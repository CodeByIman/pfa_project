May I Ask a Follow-up Question? Understanding the Benefits of
Conversations in Neural Network Explainability
TongZhang X.JessieYang BoyangLi
tong.zhang@ntu.edu.sg xijyang@umich.edu boyang.li@ntu.edu.sg
NanyangTechnologicalUniversity UniversityofMichigan NanyangTechnologicalUniversity
Singapore AnnArbor,Michigan,USA Singapore
ABSTRACT decision-making[70,95],andhelpsAIdevelopersidentifyandrec-
ResearchinexplainableAI(XAI)aimstoprovideinsightsintothe tifymodelerrors[4,58].Despitethesesuccesses,anumberofrecent
decision-makingprocessofopaqueAImodels.Todate,mostXAI studiesfindthattheexplanationsoftendonotresolveuserconfu-
methodsofferone-offandstaticexplanations,whichcannotcater sionregardingtheneuralnetworkstheyarepurportedtoexplain
tothediversebackgroundsandunderstandinglevelsofusers.With [13, 72, 77, 100, 115, 121, 134, 146]. These seemingly conflicting
thispaper,weinvestigateiffree-formconversationscanenhance findingswarrantfurtherinvestigation.
users’ comprehension of static explanations in image classifica- Wepostulatethattwomajorfactorscontributetotheineffec-
tion,improveacceptanceandtrustintheexplanationmethods,and tivenessofAIexplanations.First,theexplanationsdonotproperly
facilitatehuman-AIcollaboration.Weconductahuman-subject accountforaverageusers’knowledgeofmachinelearning,which
experimentwith120participants.Halfserveastheexperimental maybeinsufficienttoestablishcausalrelationsbetweentheexpla-
groupandengageinaconversationwithahumanexpertregard- nationsandthemodelbehaviors[53,88,100,123].Communication
ingthestaticexplanations,whiletheotherhalfareinthecontrol theorypositsthateffectivecommunicationrequiresthesendersand
groupandreadthematerialsregardingstaticexplanationsindepen- receiverstoestablishcommonground[26,27].However,experts
dently.Wemeasuretheparticipants’objectiveandself-reported usuallyfindithardtoaccuratelyestimatewhatlaypeopleknow
comprehension,acceptance,andtrustofstaticexplanations.Re- [91,135,136].Tomakemattersworse,underestimatingandover-
sultsshowthatconversationssignificantlyimproveparticipants’ estimatingthereceivers’knowledgelevelareequallydetrimental
comprehension,acceptance[31],trust,andcollaborationwithstatic tocommunication[72,136].Asaresult,theexplanationsdesigned
explanations,whilereadingtheexplanationsindependentlydoes byexpertsarealmostalwaysatamismatchwiththelaypersons’
nothavetheseeffectsandevendecreasesusers’acceptanceofexpla- actualknowledgelevel.
nations.Ourfindingshighlighttheimportanceofcustomizedmodel Second,usersofXAIhavediverseintentionsandinformation
explanationsintheformatoffree-formconversationsandprovide needs[39,53,77,134].Forexample,LiaoandVarshney[78]iden-
insightsforthefuturedesignofconversationalexplanations. tifiesfivedifferentobjectivesofusersofexplanations,including
modeldebugging,assessingthecapabilitiesofAIsystems,mak-
CCSCONCEPTS inginformeddecisions,seekingrecourseorcontestingtheAI,and
auditing forlegalor ethicalcompliance. Onestatic explanation
•Human-centeredcomputing→EmpiricalstudiesinHCI;
usuallycannotsatisfyallobjectivesandpurposes.Therefore,re-
Userstudies;•Computingmethodologies→Artificialintel-
searchershavesuggestedinjectinginteractivitytomodelexpla-
ligence.
nationsinordertoestablishcommonground,addressknowledge
gaps,andcreatecustomizedexplanationsthatadapttotheusers
KEYWORDS
[1,25,49,72,105,109].
ExplainableAI(XAI),Conversation,Interpretability,Interactive Existingworkoninteractiveexplanationscanbebroadlycate-
Explanation,Human-AIInteraction,XAIforComputerVision gorizedintotwotypes.Thefirsttype,interactivemachinelearning
[7,40],allowsuserstoprovidefeedbackandsuggestionstothe
machinelearningmodelusingmodelexplanations.Theirprimary
1 INTRODUCTION
goal is to improve machine learning performances, rather than
TherapidadvancementofArtificialIntelligence(AI)islargelypow- explainingmodelbehaviorstolaypersonusers.Inthissetting,ex-
eredbyopaquedeepneuralnetworks(DNNs),whicharedifficult planations have been shown to improve user satisfaction [122]
tointerpretbyhumans[17].Thelackoftransparencyprevents andfeedbackquality[68,76].Thesecondtypeaimstoelucidate
verificationofAIdecisionsbyhumandomainexpertsandisespe- modelbehaviorsbyallowinguserstofreelymodifyinputfeatures
ciallyconcerninginareasofhigh-stakedecisions,suchashealth- andobservehowoutputschangewhileshowingfeatureattribu-
careandlawenforcement,whereerroneousalgorithmicdecisions tionexplanations[25,56,80,126].Thistypeofinteractivityhas
couldleadtosevereconsequences[18,22,148]anderosionofpub- been shown to improve user understanding [25] and perceived
lic trust [101, 103]. To improve the explainability of AI models, usefulness[80]ofAImodels.However,theeffectiveuseofthese
numerouseXplainableArtificialIntelligence(XAI)methodshave interactiveapproachesstillrequiresarudimentaryunderstanding
beenproposed(fordetailedreviews,wereferreaderstoBodria ofmachinelearning,suchasthegenericrelationbetweeninputand
etal.[17],Danilevskyetal.[30],Yangetal.[138]).Ithasbeenre- output,orwhatmodelpropertiestheinterpretationsreveal.These
portedthatexplainabilityenhancesuserunderstanding[13]and
trust[48,87]inAImodels,improveshuman-AIcollaborationin
4202
guA
21
]CH.sc[
2v56931.9032:viXra
TongZhang,X.JessieYang,andBoyangLi
interactiveexplanationscannotanswermosttypesoffollow-up 2 RELATEDWORK
questionslaypeoplemayhave. Inthissection,wereviewthreebodiesofresearchthatmotivate
Free-form conversations that accompany static explanations ourstudy.First,weexploretheexistingworkofstaticExplainable
arearguablythemostversatilemodeofinteractionastheyallow ArtificialIntelligence(XAI).Second,wediscussinteractiveexpla-
users to ask arbitrary follow-up questions and receive explana- nations,especiallythelimitationsofexistingmethodsandtheneed
tionstailoredtotheirbackgroundsandneeds[41,72,77].Through forconversationstoenhanceexplainability.Lastly,weexamine
interviews with decision-makers, Lakkaraju et al. [72] discover differenttypesofhuman-AIcollaborationandthedesignofthe
thattheyhaveastrongpreferenceforexplanationsinnaturallan- subjectiveevaluationduringcollaboration.
guagedialogue.Theyarguethatconversationalexplanationssatisfy
fiverequirementsofinteractiveexplanationsandareidealforusers
2.1 StaticExplanation
withlimitedmachinelearningknowledge.Withtheprogressincon-
versationalcharacters[96,117,145],especiallyknowledge-based ExplainableArtificialIntelligence(XAI)referstothosemodelsthat
questionanswering[73,86,144]poweredbylargelanguagemodels canexplaineitherthelearningprocessortheoutcomeofAIpredic-
[98,129,147],AIsystemsthatcananswerquestionsabouttheir tionstohumanusers[138].StaticXAIinvolvesmodelsthatprovide
owndecisionsappeartobewithinourreachinthenearfuture. afixed,one-timeexplanation,withoutthecapabilityforfurtheruser
However,beforeinvestingefforttodevelopsuchachatbot,itwould interactionorexploration.Theyareusuallycategorizedintotwo
bebeneficialtoempiricallyquantifytheeffectsofconversational groups:self-explanatorymodelsandpost-hocmethods.Post-hoc
explanations. methodscanbecategorizedintofeatureattributionmethodsand
Inthecurrentstudy,weconductWizard-of-Ozexperimentsto example-basedmethods.Self-explanatorymodelsareinherently
investigatehowconversationsassistusersinunderstandingstatic transparent, offering clarity in their decision-making processes
explanationsofimageclassificationmodels,improvingacceptance andfacilitatingexplainability[17,30].Examplesofsuchmodels
andtrustinXAImethods,andselectingthebestAImodelsbased includelinearregression,logisticregression,decisiontrees,Naive
onexplanations.Specifically,atotalof120participantsjoinour Bayes, attention mechanism [11], decision sets [71], rule-based
experiments.Wefirstpresentthemwithstaticexplanationsforan models[107,140],amongothers.However,therequirementsof
imageclassificationtaskandmeasuretheirobjectiveunderstanding self-explanatorymodelsplaceconstraintsonmodeldesign,which
andsubjectiveperceptionsofstaticexplanations.Afterthat,half maycausethemtounderperformincomplextasks.Conversely,
oftheparticipants,whoareassignedtotheexperimentalgroup, themajorityofrecentXAImethodsarepost-hocXAImethods,
seektoclarifyanydoubtswithanonlinetextualconversationwith whichcanbeusedforanalreadydevelopedmodelthatisusually
anAIsystem,playedbyhumanXAIexperts.Theotherhalfofthe notinherentlytransparent[2,17,24,104,113,131].Thesemethods
participants,assignedtothecontrolgroup,readmaterialsaboutthe oftendonotattempttoexplainhowthemodelworksinternally,
staticexplanationsindependently.Aftertheconversationorreading but instead, employ separate techniques to extract explanatory
session,participantscompletethesamepre-sessionmeasurements. information.Post-hocXAImethodscanbeviewedasreverseen-
Fromtheresults,weestimatetheeffectsofconversationalexplana- gineeringprocessesthatemployotherindependentexplanatory
tions. modelsortechniquestoextractexplanatoryinformationwithout
Theexperimentalmeasurementsincludebothanobjectivecom- altering,elucidating,orevenunderstandingtheinnerworkingsof
ponentandasubjectcomponentoftheusers’understandingand theoriginalblack-boxmodel.Therearetwomaingroupsofmeth-
perception.Intheobjectiveevaluation,fromthreecandidateneural odstogeneratepost-hocXAIexplanations,i.e.,featureattribution
networks,theusersneedtochooseonenetworkthatwouldbethe methodsandexample-basedmethods.
mostaccurateontestdatasofarunobserved,usinginformation
fromthestaticexplanations.Thistask,knownasmodelselection,is 2.1.1 FeatureAttributionMethods. Featureattributionmethods
oneofthemostfundamentaltasksformachinelearningpractition- [6,28,57,59,82,85,104,113,116,119,124]explainmodelpredic-
ers[8].Bydesign,thethreecandidatenetworksmakeexactlythe tionsbyinvestigatingtheimportanceofdifferentinputfeaturesto
samepredictionsonthesameinputsbuthavedifferentrationales finalpredictions.Therearetwomaintypesoffeatureattribution
forthepredictions,asrevealedbythestaticexplanations.Hence, methods,gradient-basedmethods[28,85,113,119,124]andsurro-
theonlywayfortheuserstomaketherightchoiceistocorrectly gatemethods[6,57,59,82,104,116].Gradient-basedmethodsuse
understandtheexplanations.Thesubjectiveevaluationcontains gradients/derivativestoevaluatethecontributionofamodelinput
13questionsrequiringuserstoself-reportthreeaspectsoftheir onthemodeloutput.AnexamplemethodisGrad-CAM[113].It
perceptionsofthestaticexplanations:comprehension,acceptance, superimposesaheatmapontheregionsofimportantinputfeatures
andtrust. byweightingtheactivationsofthefinalconvolutionallayerby
Results show that free-form conversations with XAI experts theircorrespondinggradientsandaveragingtheresultingweights
intheWizard-of-Ozsettingsignificantlyimprovecomprehension, spatially.Besidesdirectlycalculatingtheimportancescoreofinput
acceptance,trust,andcollaborationwithstaticexplanations.Our features,severalmethodsproposetouseasimpleandunderstand-
studyunderscorestheeffectsoffree-formconversationsonneural ablesurrogatemodel,e.g.,alinearmodel,tolocallyapproximate
networkexplainabilityinpracticeandprovidesinsightsintothe thecomplexdeepneuralmodel.Surrogatemodelscanexplainthe
futuredevelopmentofconversationalexplanations.Tothebestof predictionsfromthecomplexdeepneuralmodelduetotheirinher-
ourknowledge,thisisthefirststudyofhowfree-formconversations entinterpretablenature.LIMEanditsvariantsaretypicalmethods
mayfacilitateneuralnetworkexplainabilityinpractice. forgeneratinglocalsurrogatemodels.LIME[104]buildsalinear
BenefitsofConversationsinNeuralNetworkExplainability
modellocallyaroundthedatapointtobeinterpretedandgenerates HCIresearchershaverecentlyproposedthatXAImethodsshould
aninstance-levelexplanationfortheoutput. alignwiththewayshumansnaturallyexplainmechanisms.Specif-
ically,Lombrozo[84]arguesthatanexplanationisabyproduct
ofaconversationalinteractionprocessbetweenanexplainerand
2.1.2 Example-based Methods. Example-based methods [24, 61,
anexplainee.Miller[91]arguesthatexplanationsshouldcontain
92,102,130,131]refertothosethatexplainpredictionsofblack-
acommunicationprocess,wheretheexplainerinteractivelypro-
boxmodelsbyidentifyingandpresentingaselectionofsimilar
videstheinformationrequiredfortheexplaineetounderstandthe
or representative instances. Those examples can be selected or
causesoftheeventthroughconversations.Buildingonthisperspec-
generatedfromdifferentperspectives,suchastrainingdatapoints
tiveofhumanexplanations,recentworksenvision"explainability
thatarethemostinfluentialtotheparametersofapredictionmodel
asdialogue"toprovideexplanationssuitableforawiderangeof
orthepredictionsthemselves[23,24,142],counterfactualexamples
laypersonusers[41,72,77].Whilethereismuchtheoreticalanalysis
thataresimilartotheinputquerybutwithdifferentpredictions[62,
aboutthesignificanceofconversationsinexplainability,practical
92,102,114,130,131,133],orprototypesthatcontainsemantically
investigationsintotheirimpactonusersremainlimited.Inthis
similarpartstoinputinstances[15,29,36,61,64,90].
context, two previous works have investigated the practical ef-
Inthiswork,wemainlyfocusonfeatureattributionmethods
fectofconversationsforexplainability[115,121].Shenetal.[115]
astheydirectlyhighlighttheimportanceofinputfeatures,mak-
applyconversationalexplanationstoscientificwritingtasks,ob-
ing the decision-making process of models more intuitive [65]
servingimprovementsinproductivityandsentencequality.Slack
thanexample-basedmethodsforlaypeople.Specifically,weselect
etal.[121]designdialoguesystemstohelpusersbetterunderstand
Grad-CAMfromgradient-basedmethodsandLIMEfromsurrogate
machinelearningmodelsondiabetesprediction,rearrestpredic-
methodstoconductconversationalexplanationswithparticipants.
tion,andloandefaultpredictiontasks.Despitetheseadvances,the
conversationsinthesestudiesaregeneratedbasedontemplates
andcopewithlimitedpredefineduserintentions.Inthisstudy,we
2.2 InteractiveExplanation
explore the role of free-form conversations in enhancing users’
SeveralstudiesemphasizetheneedforinteractivityinXAImeth- comprehensionofstaticexplanations,andhowtheyaffectusers’
ods[1,72,105,109].Forinstance,Lakkarajuetal.[72]findthat acceptance,trust,andcollaborationwiththeseexplanations.
decision-makersstronglypreferinteractiveexplanations.Similarly,
aliteratureanalysisbyAbduletal.[1]suggeststhatinteractions
canhelpusersprogressivelyexploreandgatherinsightsfromstatic
2.3 Human-AICollaboration
explanations.Rohlfingetal.[105]reasonthatexplanationsshould
beco-constructedinaninteractionbetweentheexplainerandthe Human-AIcollaborationisanemergingresearcharea,whichex-
explainee,adaptingtoindividualdifferencessincethehumanunder- ploreshowhumansandAIsystemscanworktogethertoachieve
standingprocessisdynamic.Fromaninterdisciplinaryperspective, sharedgoals[54,65,137].Priorstudieswithinthisdomainhavein-
SchmidandWrede[109]underscorethenecessityofuser-XAIin- vestigatedcollaborationsbetweenhumansandvariousAIsystems,
teractionstoadapttodiverseinformationrequirements. fromrobots[14,20,45,52,81]tovirtualagents[9,18,38,97].The
Tointegrateinteractivityandexplainability,twoprimarymethod- tasksinvolvedspanabroadscope,includingtext[13]andimage
ologiesemerge.Onegroupofmethodsfocusesonusingexplana- [65]classifications,medicaldiagnosis[18],deceptiondetection[70]
tions to help users provide feedback about improving machine andcooperativegames[9,42,45].Anareaofparticularinterest
learningmodels.Inthesemethods,theinteractivityliesinthecy- withinthesecollaborationsistheroleofexplanationsininfluencing
cleofmodelexplanation,userfeedback,andmodelimprovement. human-AIdecision-making[13,70,94,95].
Explanationsaimtohelpusersbetterunderstandmodeldecisions Ourstudyalignswithexistingworkonhuman-AIcollaboration
andprovidevaluablefeedback.Asaresult,machinelearningmod- [13,42,70,94,95].Inourwork,participantsneedtocollaboratewith
elscanbeincrementallytrainedwithadditionallosstermsfrom explanationstochoosethemostaccurateneuralnetworksamong
explanatoryfeedback[68,75,76,106,110,122]orwithaddeddata others.Insteadofexploringtheroleofexplanationsincollaboration,
points[5,16,127,128].However,thesemethodsareaimedatma- wemainlyexaminethepotentialofconversationsinaidingusers
chinelearningpractitionerswhocanwellunderstandandutilize toeffectivelyuseexplainabilitytechniquesandunderstandtheir
explanations. Another group focuses on enhancing user under- outputs.
standingofexplanationsbyallowingthemtomodifythemodel
inputandobservechangesinthecorrespondingoutput.Suchinter-
activityhasbeenshowntoimproveusercomprehensionandthe
3 METHOD
perceivedutilityofAImodels[25,80].Forinstance,Tenneyetal.
[126]andHohmanetal.[56]proposedifferentuserinterfacesthat Ourstudyaimstoinvestigatetheimpactofconversationsontheex-
allowfordebuggingandunderstandingmachinelearningmodels plainabilityofAImodelsbyobservingparticipants’comprehension,
byexamininginput-outputrelationships.However,arudimentary acceptance,trustofthestaticexplanations,andabilitytousethe
understandingofmachinelearningisstillrequiredforeffective explanationstoselectthemostaccurateneuralnetworksbeforeand
utilizationoftheseinterfaces,suchasthegenericrelationbetween aftertheconversation.Ourstudyhasreceivedapprovalfromthe
input and output, or what model properties the interpretations InstitutionalReviewBoardatNanyangTechnologicalUniversity
reveal. (#IRB-2023-254).
TongZhang,X.JessieYang,andBoyangLi
Table 1: Academic disciplines of our participants and the explanations.Hence,toselectthebestmodel,theparticipantsmust
numberofparticipantsineachgroup.Thereare120partici- relyontheexplanations.Figure2presentsanexamplequestion,
pantsfrom4differentdisciplinegroups. includingtheoriginalimage,themodeloutputs,andtheexplana-
tions.Thefullsetofquestionsusedinthestudycanbefoundin
AcademicDiscipline NumberofParticipants AppendixA.
Business 23 Thesubjectiveevaluationmeasuresparticipants’self-reported
Engineering 16 perceptionofthestaticexplanations,includingtheircomprehen-
Humanities 55 sion[25,55],acceptance[31,32,34,43],andtrust[31,32,34,50,141].
Science 26 Basedonanin-depthreviewofexistingliterature,wechosethe
questionsfromthosethathavebeenvalidatedinpriorresearch.
Thesubjectiveevaluationcontainsatotalof13questions,each
3.1 Participants utilizinga7-pointLikertscaleforresponses.Table2listsallthe
questionsweused.Labelsofthe7-pointLikertscalearelistedin
Atotalof120participantsjoinedourstudy.Allwere21yearsold
AppendixA.
orolder,fluentinEnglish,andhadnotbeeninvolvedinresearch
Afterthesetwoevaluations,participantsaredividedintotwo
aboutXAIpreviously.Werecruitedourparticipantsintwoways:
groups,i.e.,thecontrolgroupandtheexperimentalgroup.Partici-
bypostingadvertisementsonanonlineforumandbyemailing
pantsinthecontrolgroupreadstaticexplanationsfor15minutes.
studentsandstaffacrossvariousdepartmentsandschools.They
Participantsintheexperimentalgroupconductconversationalex-
arefromawiderangeofdisciplinestopromotediversity.Forease
planationswithparticipantsintheWizard-of-Oz(WoZ)setting
ofreporting,wecategorizetheirdisciplinesintofourgroups:
[63].Theyinteractwithadialoguesystemthattheybelievetobe
• Business,includingBusinessandAccountancy.
autonomousbutisactuallyoperatedbyahumanexpertonmachine
• Engineering,includingCivilandEnvironmentalEngineer-
learning.
ing,ComputerScience,ElectricalandElectronicsEngineer-
TosupporttheWoZexperiment,webuiltaconversationpage
ing,MaritimeStudies,andFoodScience.
withatwo-sectionstructure,asdepictedinFigure3.Ontheleft,
• Humanities,includingPsychology,Economics,Communi-
the page shows a task description, a textual description of the
cationStudies,LinguisticsandMultilingualStudies,and
predictionmodel,atextualdescriptionoftheexplanationtechnique,
Sociology.
anexampleinputimage,themodelpredictionontheinputimage,a
• Science,includingBiology,Chemistry,ChemicalEngineer-
staticexplanationfortheprediction,andatextualdescriptionofthe
ingandBiotechnology,SportScience&Management,Math-
explanation.Ontheright,theinterfaceenablesuserstoconverse
ematics,Medicine,andPhysics.
withXAIexperts,seekingclarificationsandposingquestionsabout
Table1showsstatisticsoftheacademicdisciplinesthatthepartici- theexplanation.Fortheusersinthecontrolgroup,wereplacethe
pantsenrolledin. textualchatuserinterfacewitha15-minutetimer.Oncethetimer
reacheszero,usersareallowedtoproceedtothepost-evaluations.
3.2 ExperimentalTask
Usersfrombothgroupsreceivethesamepost-evaluations,which
Inourstudy,wefocusontheimageclassificationtaskontheIma- areidenticaltothepre-evaluations. Wediscusstheevaluations
geNetdataset[33].Imageclassificationtaskisacornerstoneinthe below.
fieldofcomputervision(CV)thathasbeenthesubjectofvarious
human-AIcollaborativestudies[61,125].Wetrainthreeclassifi-
3.3 ExperimentalDesign
cationmodelswithdifferenttop-1classificationaccuracies:Swin
Transformer[83](84.1%),VGG-16[120](71.6%),andAlexNet[67] Therearetwoindependentvariablesandtwocategoriesofdepen-
(56.5%).Togenerateexplanationsformodelpredictions,weselect dentvariables.Theindependentvariableintheexperimentsisthe
twoexplanationtechniquesfromtwomaincategoriesoffeature explanationmethod:LIMEorGrad-CAMandthemethodofun-
attributionexplanationmethods:LIME[104](asurrogatemethod) derstandingstaticexplanations:conversationwithhumanexperts
andGrad-CAM[113](agradient-basedmethod).Wefocusonfea- orreadingstaticexplanations.Aswedevisebothsubjectiveand
tureattributionexplanationsaswebelievetherelationshipbetween objectiveevaluationsbeforeandafterconversationsorreadings,
inputfeaturesandmodelpredictionsismoreintuitivetounder- twocategoriesofdependentvariableswerecollectedintheexperi-
stand than example-based methods for laypeople [65]. Figure 1 ment:themodelselectionaccuracyandtheself-reportedperception
displaysexampleexplanationsgeneratedbythesetwoexplanation scores.
methods.
Toconductthestudy,wedesignandbuildaweb-basedplatform 3.3.1 ObjectiveEvaluation–SelectionofClassificationModels. The
whereparticipantscanremotelyfinishthewholeprocedureofthe evaluationaimstoobjectivelyevaluateparticipants’understanding
experiment.Afteruserslogintotheplatform,wefirstevaluatetheir ofthestaticexplanations.Participantsarepresentedwith5input
objectiveandsubjectiveunderstandingofstaticexplanations.The images,onwhichthethreeneuralnetworksmakethesamedeci-
objectiveexplanationsrequireparticipantstochoose,fromthree sions.Theonlydifferencesbetweenthethreenetworkslieintheir
classificationmodels,themostaccurateonunobservedtestdata. explanations.Participantsneedtochoosetheonethatwouldbe
Thethreeclassificationmodelsyieldidenticaldecisionson5im- themostaccurateonunobservedtestdata.Hence,tomakethecor-
ages.Theonlydifferencesbetweenthethreenetworkslieintheir rectselection,theparticipantsmustunderstandtheexplanations.
BenefitsofConversationsinNeuralNetworkExplainability
(a)Inputtotheclassificationmodel(Swin (b)ExplanationgeneratedbyGrad-CAM (c)ExplanationgeneratedbyLIME
Transformer)
Figure1:ExampleexplanationsgeneratedbyGrad-CAMandLIME.(a)istheinputtotheclassificationmodel(SwinTransformer),
(b)istheexplanationgeneratedbyGrad-CAM,and(c)istheexplanationgeneratedbyLIME.Thepredictedclassofthemodelis
"Siamesecat".
Figure2:Anexampleoftheobjectiveevaluation.Theobjectiveevaluationaimstoobjectivelymeasureparticipants’comprehen-
sionofstaticexplanations.Eachchoicecontainsapredictionfromadifferentclassificationmodel,pairedwithitsrespective
staticexplanation.Participantsneedtochoosethebestmodelbasedontheexplanations.
We use the accuracy of selecting the correct model to measure provideactionableinformationformodelselection.Asourgoalis
participants’objectiveunderstandingofstaticexplanations. totestiftheuserscanunderstandthestaticexplanationswhenthey
Werecognizethatexistingexplanationtechniquesarenotal- doprovideactionableinformation,ratherthanevaluatingthestatic
waysfaithfultotheunderlyingmodel[3,60,66]anddonotalways explanationsthemselves,weselectedinputimageswherebetter
TongZhang,X.JessieYang,andBoyangLi
Table2:Detailedquestionsinthesubjectiveevaluation.Theuserwillrespondtoeachquestionusinga7-pointLikertscale.
Aspect Question
Howmuchdoyouthinkyouunderstandtheexplanationsprovidedforpredictionsofdeeplearning
Comprehension
models?
Usingexplanationswouldimprovemyunderstandingofdeeplearningmodels’predictions.
Usingexplanationswouldenhancemyeffectivenessinunderstandingpredictionsofdeeplearning
Perceived models.
Usefulness
Iwouldfindexplanationsusefulinunderstandingpredictionsofdeeplearningmodels.
IbecomeconfusedwhenIusetheexplanationinformation.
Perceived Itiseasytouseexplanationinformationtounderstandpredictionsofdeeplearningmodels.
Ease-of-Use
Overall,Iwouldfindexplanationinformationeasytouse.
Iwouldprefergettingexplanationinformationaslongasitisavailablewhengettingpredictionsfrom
Behavioral deeplearningmodels.
Intention Iwouldrecommendothersuseexplanationinformationtounderstandpredictionsofdeeplearning
models.
Howwouldyouratethecompetenceoftheexplanationmethod?-i.e.towhatextentdoestheexplana-
tionmethodperformitsfunctionproperly?
Howwouldyouratethedependabilityoftheexplanationmethod?-i.e.towhatextentcanyoucount
ontheexplanationmethodtoexplainpredictionsofdeeplearningmodels?
Trust
Howwouldyourateyourdegreeoffaiththattheexplanationmethodwillbeabletoexplainpredictions
ofdeeplearningmodelsinthefuture?
Howwouldyourateyouroveralltrustintheexplanationmethod?
classificationmodelsindeedhavemorereasonableandintuitive deep learning models. Along with perceived ease of use and
explanations.Thisapproachallowsuserstoeasilypickthebest behavioralintention,thesethreeaspectsmeasureparticipants’
classificationmodelsiftheyunderstandthestaticexplanationswell. acceptanceofstaticexplanations.Theyarederivedfromthe
Wedeemanexplanationmorereasonablewhenitfocusesmoreon TechnologyAcceptanceModel(TAM)[31,32,34],awidelyap-
discriminativefeaturesthatareuniquetothepredictedclassand pliedtheoryforunderstandingindividualacceptanceandusage
lessonspuriousfeaturesthatareirrelevanttotheclass.Inaddition, ofinformationsystems.Astheexplanationsareusedbyend-
goodmodelsshouldhaveexplanationsthatrelyonmultipletypes users,investigatingtheiracceptanceoftheexplanationsisvery
ofdiscriminativefeatures.Thisisbecauseamodelrelyingonmul- important.
tiplefeaturesisrobustandmakesthecorrectdecisionevenifsome • PerceivedEaseofUse[31,32,34]:Participants’assessmentof
discriminativefeaturesaremissingoroccluded.Intheexamplein thesimplicityandclarityoftheexplanations.
Figure2,ModelBisbetterthanModelAorModelCasModelB • BehavioralIntention[31,32,34]:Thetendencyofparticipants
utilizesboththeheadandthebodyofthecatforclassification.Inad- toutilizetheexplanationinformationinthefuture.
dition,unlikeModelA,ModelBdoesnotfocusonthebackground, • Trust[10,93]:Participants’confidenceintheexplanationmeth-
whichisirrelevanttothepredictedclass,SiameseCat. odskeepingfunctioningasintended.Trusthasbeenrecognized
asanimportantfactorinhuman-AIcollaborationasitmediates
thehuman’srelianceonAImodels,thusdirectlyaffectingthe
3.3.2 SubjectiveEvaluation. Wealsomeasureparticipants’subjec-
effectivenessofthehuman-AIteam[35,111,112,118,132].
tiveperceptionofstaticexplanations,includingtheircomprehen-
Theliteraturedemonstratedthatstaticexplanationshavein-
sion,acceptance,andtrust.Thesubjectiveevaluationcontainsa
consistenteffectsonusers’trustinAIsystems.Ononehand,
totalof13questionslistedintable2.Allquestionsutilizea7-point
severalstudieshavedemonstratedthatdetailedexplanations
Likertscaleforresponses.
[46,51,118],contrastiveexplanations[74],andexample-based
• Comprehension[25,55]:Participants’subjectiveperceptionsof
explanations[139]canenhanceusertrustinsystems.Onthe
theirunderstandingofexplanations.Itcomplementstheobjec-
otherhand,studiesshowedthatstaticexplanationsdonothave
tiveevaluation,providingaholisticperspectiveonparticipants’
strongeffectsonusertrustinAIsystems[25,69,134,146].
understandingofstaticexplanations.
Onemainreasonfortheseinconsistentreportsisthattrustis
• PerceivedUsefulness[31,32,34]:Thedegreetowhichpartici-
mediatedbytheusers’understandingofthestaticexplanations
pantsfeelthattheexplanationsenhancetheirexperiencewith
BenefitsofConversationsinNeuralNetworkExplainability
Figure3:Thewebpagewhereuserscandiscussstaticexplanationswithanexpert.
[69,134,146],andsuchunderstandingisoftenabsent.Accord- 3.4 DetailedStudyProcedure
ingtotheoriesoftrust[55,79,89],theabilitytobuildamental Beforeparticipation,individualsarerequiredtosignaninformed
modelofAIsystemsisthekeyforusertrustinAI.Unsurpris- consentformthatoutlinestheobjectivesandproceduresofthe
ingly,studiesontheeffectsofstaticexplanationsforlaypersons study. The form also clarifies compensation details and assures
showthatuserswithlimitedknowledgeofmachinelearning boththeanonymityandconfidentialityofdatacollectedduringthe
struggle to understand static explanations and the decision- study.Uponsigningtheconsent,participantsreceiveanemailthat
makingprocessestheyaresupposedtoexplain.Consequently, guidesthemtoaccessthestudyplatform.
theseusersdonotexhibitincreasedtrustinAIsystemsafter Afterloggingin,apop-uppromptprovidesanoverviewofthe
receivingstaticexplanations[134,146]. tasksahead.Participantsthencompletepre-experimentobjective
With this paper, we quantitatively investigate whether cus- andsubjectiveevaluationsofthestaticexplanations.Theobjective
tomized conversations about static model explanations can evaluationmeasuresparticipants’understandingofstaticexplana-
enhanceuserunderstandingandimprovetrust.Theconver- tionsbylettingthemchoose,fromthreeclassificationmodels,the
sationalapproachtowardexplanationshasbeenadvocatedby mostaccurateonunobservedtestdata.Thereare5explanationex-
previousstudies[41,46,72,99,108]butneverexperimentally amplesintheobjectiveevaluation.Thesubjectiveevaluation,with
verified.Forexample,throughinterviewswithdecision-makers, 13self-reportingquestions,probestheperceivedcomprehension,
Lakkarajuetal.[72]foundthatdecision-makersstronglyprefer acceptance,andtrusttowardsthestaticexplanations.Following
conversationalexplanationsthatallowthemtoaskfollow-up theseevaluations,participantsintheexperimentalgroupengage
questions. inaWoZdiscussionaboutstaticexplanations.Duringtheconver-
sation,oneexampleimageisdisplayedonthescreen.Theexample
TongZhang,X.JessieYang,andBoyangLi
image is different from those used in the evaluations; however, WeobservevariedobjectiveperformancebetweenLIMEand
theexplanationmethodsremainthesame.Participantsaremoti- Grad-CAM(𝐹(1,116) = 218,𝑝 < .001).Grad-CAMhasahigher
vatedtounderstandtheexplanationsastheyneedtoselectthe accuracyofobjectivedecisionaccuracycomparedtoLIME.Apo-
best-performingclassificationmodelusingexplanationsonlywhen tentialreasonmightbetheinherentlyintuitivenatureoftheexpla-
doingobjectiveevaluation.OurXAIexpertsfaithfullyanswerthe nationsproducedbyGrad-CAMcomparedtoLIME.
user’squestionsbasedontheirknowledge,tryingtohelptheuser
graduallyunderstandtheexplanation.Forparticipantsinthecon-
trolgroup,theyreadthestaticexplanationfor15minuteswhich
istheaverageconversationtimeoftheexperimentalgroup.After
theconversationor15-minutereading,participantscompletethe
samesetofevaluationsasbefore.Allevaluationoutcomesandcon-
versationrecordsaredocumented.Uponstudycompletion,each
participantreceivesa$10reward.
4 RESULTS&DISCUSSION
Table3tabulatesthemeanandstandarddeviation(SD)forallthe
measures.Asexplanationmethods(LIMEvs.Grad-CAM)andgroup
(experimentalvs.control)arebetween-subjectsvariablesandtime
(beforevs.after)isawithin-subjectvariable,weconductathree-
wayAnalysisofVariance(ANOVAs).
4.1 Effectsofexplanationsonobjectivedecision
(a)LIME
accuracyandsubjectivemeasures
Resultsshowsignificantmaineffectsofgroup(𝐹(1,116)=5.60,𝑝 =
.02), method (𝐹(1,116) = 218,𝑝 < .001) and time (𝐹(1,116) =
12.51,𝑝 <.001).Theexperimentalgroup,theGrad-CAMmethod,
andtheafter-conversationconditiondisplayahigherobjectivedeci-
sionaccuracy.Wealsofindasignificantinteractioneffectbetween
groupandtime(𝐹(1,116)=11.3,𝑝 =.01),asdisplayedinthefigure
4.Intheparticipant’sinitialdecision,therewerenosignificant
differencesbetweentheexperimentalandcontrolconditions.Dur-
ingparticipants’finaldecision,thosewhointeractwiththeXAI
expert(i.e.,experimentalcondition)havebetterdecisionaccuracy.
Thisphenomenonhighlightstheeffectivenessofconversational
explanationsinenhancingtheobjectiveunderstandingofstatic
explanationsofusers.
(b)Grad-CAM
Figure5:Subjectiveunderstandingscorefor(a)LIMEand(b)
Grad-CAMbeforeandafterconditions.
Intermsofparticipants’subjectiveunderstanding,wefindasig-
nificantmaineffectoftheevaluationtiming(𝐹(1,116)=4.08,𝑝 <
.001).Participantsreceivingconversationalexplanationshavea
significantlylargerimprovementinsubjectiveunderstanding.We
alsoobserveasignificantinteractioneffectbetweengroupandtime
(𝐹(1,116)=37.3,𝑝 <.001),showninfigure5.Initially,thereisno
significantdifferenceintheparticipants’self-reportedunderstand-
ingofstaticexplanationsbetweentheexperimentalandcontrol
groups.Aftertheconditions,participantsintheexperimentalgroup
demonstrateahigherself-reportunderstandingcomparedtothose
inthecontrolgroup.
Figure4:Objectivedecisionaccuracyfordifferentgroups Themaineffectoftheexplanationmethod(𝐹(1,116)=.72,𝑝 =
beforeandafterconditions. .40)isnotsignificantforparticipants’subjectiveunderstanding,
BenefitsofConversationsinNeuralNetworkExplainability
Table3:Resultsoftheexperimentalgroupbeforeandafterconversations,andthecontrolgroupbeforeandafter15-minute
reading.Eachscoreispresentedasmean±standarddeviationandthechange𝛿beforeandafter.∗𝑝 <0.001
Objective
Explanation Evaluation Understanding Subjective Perceived Perceived Behavioral
Group Trust
Methods Timing (Decision-Making Understanding Usefulness EaseofUse Intention
Accuracy)
before 0.38±0.20 4.03±1.35 5.09±1.07 4.48±0.94 5.25±0.95 4.15±0.88
experimental after 0.53∗±0.16 5.30∗±0.88 5.92∗±0.66 5.28∗±0.84 5.83∗±0.81 4.92∗±0.73
LIME
before 0.37±0.17 4.57±1.43 5.67±0.95 4.87±1.26 5.73±0.69 4.37±0.90
control
after 0.40±0.20 4.60±1.16 5.33±0.96 4.48±1.26 5.27±1.08 4.36±1.05
before 0.82±0.21 4.17±0.91 5.49±0.97 4.71±0.95 5.52±0.65 4.40±1.00
experimental after 0.92∗±0.11 5.43∗±0.97 6.12∗±0.60 5.58∗±0.82 6.08∗±0.79 5.19∗±0.80
Grad-CAM
before 0.81±0.20 4.07±1.34 5.58±0.59 4.36±1.15 5.45±0.71 4.22±0.96
control
after 0.79±0.19 4.40±1.28 5.46±0.69 4.70±1.21 5.33±0.83 4.42±0.87
contrastingwithitseffectonobjectiveunderstanding.Eventhough receivingconversationalexplanations.Onthecontrary,thebehav-
participantscanintuitivelychoosethebestclassificationmodel ioralintentionofthecontrolgroupdecreaseforbothGrad-CAM
basedontheheatmapintheobjectiveevaluation,participants’ini- andLIME.
tialself-reportingunderstandingscoreofGrad-CAMisjustslightly Theboostinusefulness,easeofuse,andbehavioralintention
largerthan4(averageunderstanding).Thisshowsthatparticipants fortheexperimentalgroupcanbeattributedtotheincreasedun-
stillfeelconfusedabouthowGrad-CAMworksandhowitexplains derstandingofstaticexplanations.Priortotheexpertinteractions,
models’ predictions, even though they can perform well in the participantsmighthavehadlimitedknowledgeorevenmisconcep-
objectiveevaluation.Thisalsodemonstratesthatsubjectiveand tionsabouttheexplanationmethods.Experimentresultsshowthat
objectiveevaluationsmeasureparticipants’understandingofstatic participantsgainaclearerunderstandingofhowtheXAImeth-
explanationsfromcomplementaryaspects.Self-reportingscores ods function, after the participants’ questions are addressed in
canbeinfluencedbypersonalbiases,whiletheobjectiveevaluation theconversations.Consequently,theyreportperceivingthestatic
mightnotcaptureusers’feelingsaboutunderstanding.Combining explanationsasmoreusefulandeasiertouse,andreporthigher
bothmethodscanprovideacomprehensiveviewofparticipants’ inclinationtousethestaticexplanationsinfuturetasks.
understandingofstaticexplanations. Theperceivedusefulness,easeofuse,andbehavioralintention
Fortheperceivedusefulness,resultsshowasignificantmain ofthecontrolgroupalldecreaseafterreadingstaticexplanations
effectoftime(𝐹(1,116)=14.6,𝑝 <.001),aswellasasignificantin- foralongertime.Thistrendsuggestsadecreasedwillingnessto
teractioneffectbetweengroupandtime(𝐹(1,116)=52.9,𝑝 <.001), utilizeexplanationsinfuturescenarios.Thereluctancemaybeat-
asdepictedinfigure6.Theexperimentgroup(i.e.,receivingcon- tributedtothefrustrationthecontrolgroupfacedinattempting
versationalexplanation)resultsinalargerincrementofperceived tocomprehendthestaticexplanationsontheirown.Researchby
usefulness.Forthecontrolgroup,theGrad-CAMmethodincreases Carolin Ebermann and Weibelzahl [21] on the impact of cogni-
perceivedeaseofusewhenparticipantsaregivenmoretimeto tivefitandmisfitintheacceptanceofAIsystemusagehighlights
viewthestaticexplanation.However,areversedtrendisobserved thisphenomenon.Theyfoundthatusersexperiencingacognitive
fortheLIMEmethodinthecontrolgroup–theperceivedeaseof misfitwiththeAIsystemoftenreportnegativemoods,whichin
usedropsafteradditionaltimeisprovided. turn,reducetheirperceivedusefulness,easeofuse,andbehavioral
Similar results are observed for participants’ perceived ease intentionoftheAIsystems.Thecontraryresultsofthecontrol
of use. There are significant main effects of group (𝐹(1,116) = groupandtheexperimentalgroupalsounderscoretheimportance
5.19,𝑝 =.002)andoftime(𝐹(1,116)=30.3, 𝑝 <.001),aswellasa andeffectivenessofconversationsinenhancinguserbehavioral
significantinteractioneffectbetweengroupandtime(𝐹(1,116)= intentionsofstaticexplanations.
33.7,𝑝 <.001).Theperceivedeaseofuseincreaseslargelyforthe For the trust, results show significant main effects of group
experimentgroupafterinteractingwithXAIexperts.Forthecon- (𝐹(1,116)=4.31,𝑝 =.04)andtime(𝐹(1,116)=70.0,𝑝 <.001).The
trolgroup,theGrad-CAMmethodincreasesperceivedeaseofuse experimentalgroupandtheafterconditiondisplayahighertrust
whileLIMEmethodsdecreaseitwhengivingparticipantsmore scoreofparticipants.Wealsofindasignificantinteractioneffect
timetoviewthestaticexplanation. betweengroupandtime(𝐹(1,116)=43.7,𝑝 <.001),asdisplayed
Forthebehavioralintention,resultsshowasignificantmain inthefigure9.Initially,therewerenosignificantdifferencesin
effectofthetime(𝐹(1,116)=3.92,𝑝 =.005)andasignificantinter- trustscoresbetweentheexperimentalandcontrolconditions.Dur-
actioneffectbetweengroupandtime(𝐹(1,116)=3.92,𝑝 <.001)as ingparticipants’finaldecision,thosewhointeractwiththeXAI
showninfigure8.Participantsincreasetheirbehavioralintention expert(i.e.,experimentalcondition)reportahighertrustscore.The
andaremoreinclinedtouseexplanationsinfuturescenariosafter
TongZhang,X.JessieYang,andBoyangLi
(a)LIME (a)LIME
(b)Grad-CAM (b)Grad-CAM
Figure 6: Participants’ self-report usefulness score for (a) Figure7:Participants’self-reporteaseofusescorefor(a)
LIMEand(b)Grad-CAMbeforeandafterconditions. LIMEand(b)Grad-CAMbeforeandafterconditions.
enhancementsoftheexperimentalgroup,contrastedwiththeun-
e.g.,whatisadeeplearningmodel,whatisaccuracy,themodel
changedtrustscoreofthecontrolgroupindicatethatinformative-
structure,andthetrainingdata,etc.
nessandclaritythroughconversationscanhelpstaticexplanations
• Applicationandperformanceofmachinelearningmodels:Ques-
gainmoretrustfromusers.Whilethereexistnumerousstudies
tionsabouttheability,accuracy,andlimitationsofmachine
onhowexplanationsofAIpredictionscaninfluenceusers’trust
learning.
inAIpredictions[25,65,69,88,143,146],toourknowledge,this
• Diagram reading: Questions about the explanation diagram
isthefirstexperimentdesignedexplicitlytogaugetheimpactof
generatedbyGrad-CAMorLIME,e.g.,whatdifferentcolors
conversationsonenhancingparticipants’trustinexplanations.
representintheheatmap.
4.2 AnalysisofCollectedConversations • BasicconceptsinexplainableAI:Questionsaboutbasicconcepts
ofexplanationmethods,e.g.,whatareexplanationmethods?
Wecollect60free-formconversationsbetweenXAIexpertsand
• Mechanismofexplanationmethods:Questionsabouthowex-
participantsfrom4differentdisciplinegroups.Onaverage,each
planationmethodsworkandhowtheprovidedexplanationis
conversationhad27.4turns,witheachturncomprisingapproxi-
generated.
mately14.4tokens.Byanalyzingtheusers’questions,wedivide
• Otherexplanations:Questionsthatrequirethegenerationof
themintosixcategories:
othertypesofexplanationsonthecurrentpredictions,expla-
• Basicconceptsinmachinelearning:Questionsaboutbasicterms nationsfordifferentpredictions,orcomparisonsbetweenthe
andconceptsinmachinelearningthatlaypeoplemaynotknow, providedexplanationandotherexplanationmethods.
BenefitsofConversationsinNeuralNetworkExplainability
(a)LIME (a)LIME
(b)Grad-CAM (b)Grad-CAM
Figure8:Participants’self-reportbehavioralintentionscore Figure9:Participants’trustfor(a)LIMEand(b)Grad-CAM
for(a)LIMEand(b)Grad-CAMbeforeandafterconditions. beforeandafterconditions.
Basedonthiscategorization,webuildarepositoryforquestions byGrad-CAMandLIMEwererelativelystraightforwardandeasy
thatcouldoccurintheconversations.Intotal,wecollected397 tounderstand.Thediverserangeofquestionssourcedfromour
questionsfromthefourdifferentcategories.Table4containsexam- conversationsunderscoresthatstatic,one-offexplanationsareof-
plesandthenumberofquestionsineachcategory.Asobservedin teninsufficientforuserstounderstandthem.Engagingindialogue
Table4,thequestionsofparticipantsmainlyrevolvearoundbasic canprovidemoredynamicandtailoredexplanationstousers,hence
concepts in machine learning, the fundamentals of explanation deepeningtheirunderstandingofstaticexplanations.
methods,andtheirunderlyingmechanisms.Thistrendmightbe Havingwellinternalizedtheirknowledge,expertsareoftenun-
attributedtothemulti-disciplinarityoftheparticipants.Itsuggests abletoestimatewhatlaypeopleknow[136].Thisphenomenonis
thatmanyparticipantsmaynotbefamiliarwithmachinelearning alsoreferredtoasthe“curseofknowledge”[19].Asaresult,experts
modelsandexplanationmethods,whichisalignedwiththereal tendtooverlookpotentialareasofconfusionormakeunwarranted
applicationofexplanationmethods.Therefore,it’scrucialtotai- assumptionsaboutwhatis“commonknowledge”.Whileanalyzing
lorresponsestothesequestionstohelpusersbetterunderstand thecollectedconversations,weoftenfindourselvesunabletoan-
explanations.Furthermore,wenoteamarkedinterestinnewex- ticipatetheuserquestions,whichcorroboratestheliterature.We
planations.Thiscouldindicatethatasusersbecomemorefamiliar describeafewexamplesbelow.
withprovidedexplanationexamples,theyexhibitcuriosityabout Severalparticipantsmisunderstoodtheideaoftheheatmappro-
alternativeexplanationmethodsandhowmodelsmightbehaveun- ducedbyGrad-CAMasdepictingliteralheatdissipatingfromob-
derspecificscenarios.Concurrently,thediagramreadingcategory jects.Theyinferthatthemodelusesthetemperatureofobjects
containsonly16questions,implyingthatexplanationsgenerated toperformclassification.Inreality,aheatmapisjustametaphor
TongZhang,X.JessieYang,andBoyangLi
thatvisualizesnumericalvaluesdistributedspatially,whichrefers Besides,inourstudy,participantsalsoarecuriousaboutthe
tothefeatureimportanceinourcase.Thismisconceptionleads applications,performances,andlimitationsofmachinelearning
toquestionsabouthowtheheatofobjectsismeasuredandwhy modelsandexplanationmethods.Therefore,besidesanswering
non-livingobjectsarewarmerthantheirenvironment.Someexam- abstractquestions,dialoguesystemsalsoshouldrelatethemto
pleutterancesfromparticipantsinclude:“SotheGrad-cammethod real-worldapplicationsandlimitations.
basicallyjustreferstotheusageofgeneratingaheatmaptocapture • Capabilitytogeneratenewexplanationsasneeded.Asanim-
livingmatterscorrect?...basedonthepartsoftheimagethatgenerate proved understanding of the provided explanations, partici-
moreheat?”–P36,“basicallyusingheattopredictwhatistheinput pantsinourstudyexhibitcuriosityaboutalternativeexplana-
right?...howwillweknowwhatistheanimalorinputsimplybased tionmethodsandexplainingdifferentpredictions.Dialogue
onheat?”–P47,“ifthesearepictures,howdotheyfigureouttheheat systemsshouldprovidenewexplanationstouserswhenre-
sincetheanimalisn’tgeneratingheat”–P49,“Soaheatsensorisnot quested.Forinstance,ifauseriscuriousabouthowchanginga
required?Aheatmapisautomaticallygeneratedfromeachphotoand featurewouldaffectthemodeloutput,thesystemshouldgener-
analyzedusingthemodel.”–P52. ateanewexplanationwiththenewfeature,whichshowcases
Asecondcommonmisconceptionistheconflationbetweenthe theeffect.
post-hocexplanationtechniqueandtheclassificationmodels.Some • Capabilitytointerpretscientificdiagramsandvisualizations.A
exampleuserquestionsinclude:“istheexplanationmethodwhatthe significantportionofAIandXAIexplanationsoftencomesin
modelusestoclassify&predictwhattheimageissupposedtobe?”– theformofdiagrams[104,113],suchasheatmapsorfeature
P6,“SwintransformerusesLIMEmodel?...whatarethedifferences importancevisualizations.Ourstudyrevealsthatusershave
betweenlimemodelandSwintransformer?”–P8.Furthermore,par- questionsrelatedtounderstandingthesediagrams.Answering
ticipantsfacechallengesinunderstandingcertaintermscommonly thesequestionsusuallyrequiresanunderstandingofspecific
usedinAIandXAI,eventhoughthesetermsarefrequentlyused regionsofthediagrams,suchasansweringwhatpartsofthe
andunderstoodwithinacademiccommunities.Manyparticipants objectarehighlightedbytheyellowlineinLIMEexplanations.
askedquestionsaboutbasicconceptsinmachinelearning,such Therefore,futuredialoguesystemsshouldhavevisualprocess-
as:“whatistheexplanationmethod?”–P7,“howdoyouclassify ingcapabilities,understandingandinterpretingdiagramscon-
theimage?”–P17,“whatisthealgorithm?doesitmeanlime?what textually.Forinstance,theyshouldbeabletorecognizecolors,
aredeepneuralnetworks?”–P32,“Howwouldyouexplaintheterm patterns,andothergraphicalelementsinheatmapsorcharts
"perturbationsofimages"toafive-year-old?”–P46. andrelatethemtousers’questions.Therecentdevelopmentin
TheobservationsfromtheinteractionsbetweenXAIexpertsand multimodallargelanguagemodels[37,47,149]isapromising
laypersonusersdemonstratetheimportanceofconversationsfor directiontoachievethisgoal.
userstounderstandstaticexplanationsastheybridgetheknowl-
edgegapbetweenthetwogroups.Conversationscanrevealthe
4.4 Limitations
specificareasofmisunderstanding,suchasincorrectimplicitas-
sumptionstheusersmakeandknowledgetheylack.Hence,conver- Despitetheinsightsgained,thereareseverallimitationsthatshould
sationalexplanationsmayhelptheAIsystemcommunicatewith beacknowledged.First,thestaticexplanationsusedinourstudyare
andbringgenuineunderstandingtotheusers. limited.Ourexperimentsfocusedonfeatureattributionexplanation
methods.Theapplicabilityofourfindingstootherexplanation
4.3 Implicationsforbuildingdialoguesystems methods,suchasexample-basedexplanationmethods,remainsan
toexplainstaticexplanations openquestion.Second,asourmainobjectivewastodiscernthe
effectsoffree-formconversationalexplanations,wedidnotdelve
Ourstudyindicatestheimpactofconversationalexplanationson
intothecomparativeperformanceofdifferentexplanationmethods.
usercomprehension,acceptance,andtrustofstaticexplanations.
Inourexperiments,weintentionallyselectedexplanationexamples
Staticexplanations,whileinformative,maynotcatertouserswith
wherethebestclassificationmodelyieldedthemostreasonable
variedbackgroundsandexpertise.Engaginginconversationalex-
explanations.Theexplanationexamplesdiscussedbyparticipants
planationsprovidesadynamicandinteractivemediumforusers
andXAIexpertswerechosensuchthattheyreasonablyexplain
toseekclarifications,askquestions,andtherebyfacilitateadeeper
thepredictionsoftheclassificationmodel.Futureworkwouldbe
andmorepersonalizedunderstanding.
toextendtheseconversationstoincludeexplanationsthatmight
Theemergenceofadvancedconversationalagents[96,117,145],
belessreliable.Third,weexplorehowconversationsfosteruser
especiallyknowledge-basedquestion-answering[73,86,144]pow-
trustinexplanationsinourstudy.Nevertheless,previousstudies
eredbylargelanguagemodels[98,129,147]pavesthewaytoward
[51,134,146]haveshownthathumansmaytrustAImodelseven
conversationalagentsthatcanexplainmodeldecisionsanddiscuss
iftheymakewrongdecisions.Wedonotexplorewhetherusers’
staticexplanations.Ourstudysuggeststhefollowingdesideratafor
trustinourstudyismisplaced,whichweleaveforfuturework.
suchagents.
Fourth,weuseAItoclassifytheimages.Previousstudies[12,44]
• ExtensiveknowledgeofAIandXAI.Asobservedinourstudy,a found that participants favor humans over AI decision-makers
largeportionofuserquestionsarerelatedtocoreconceptsof whentheirdecisionsdirectlyaffectparticipantwelfare.Inourstudy,
machinelearningmodelsandexplanationmethods.Toanswer AIdecisionsdonotdirectlyaffectparticipantwelfare.Wealsodid
thosequestions,conversationalagentsneedtobetrainedon not investigate if the participants preferred conversations with
acomprehensivecorpusencompassingAIandXAIconcepts. humansorAIchatbotsoriftheirtrustintheexplanationswas
BenefitsofConversationsinNeuralNetworkExplainability
Table4:OverviewofCollectedQuestions.Includingcategoriesofquestions,examples,andthecountofquestionsineach
category.
QuestionCategory QuestionExamples Num
• Whatisadeeplearningmodel?
Basicconceptsin
• Whatistheimageclassificationtask? 85
machinelearning
• Howdoesthemodelknowwhatfeaturestoextract?
Application,
• Howabouttheprecisionoftheclassificationmodel?
performance,and
• WherehasthisSwinTransformerclassificationmethodbeenusedinpracticalapplications? 68
limitationsofmachine
• Willthedifferentspeciesofananimalaffecttheclassificationmodelcategorizingtheanimal?
learningmodels
• Areregionscoloredinredareasthathavebeenidentifiedascontainingkeyfeaturesfortheanimal?
Diagramreading • Whataretheyellowlinespotsfor(inLIMEexplanations)? 16
• Whatdotheredandbluecolorsmean(inGrad-CAMexplanations)?
• Whatistheexplanationmodelusedfor?
Basicconceptsof
• CanLIMEbeusedwithouttheinternet? 95
explanationmethods
• WhataresomelimitationsoftheGrad-CAM(LIME)method?
• Whydoesthe(LIME)explanationnothighlightallthepartsoftheleopard?
Mechanismof • HowLIMEmodelrecognizethemostimportantpartsforthemodelprediction?
91
explanationmethods • SeemsliketheClassificationModelandtheExplanationModelaretrainedseparately-howcanwe
besurethattheunderlyinglogicofmakingapredictionisthesameforbothmodels?
• Canyoulistothervisualizationmethods?
• IsthereanythingspecialabouttheGrad-CAM(orLIME)methodthatisdifferentfromothers?
Otherexplanations 42
• Whatiftherearebothfishesandhumansinanimage?Howshouldthisimagebeclassified,andcan
youprovidesuchexplanations?
affectedbythatvariable.Finally,ourresearchisconfinedtoone Systems:AnHCIResearchAgenda.InProceedingsofthe2018CHIConferenceon
geographicalregionandincludesonlystudentsandstafffromthe HumanFactorsinComputingSystems.AssociationforComputingMachinery,
1–18.
university.Factorssuchasculturalbackgroundsandage-related
[2] AminaAdadiandMohammedBerrada.2018.Peekinginsidetheblack-box:A
differencescouldpotentiallyinfluenceuserinteractionswithXAI surveyonexplainableartificialintelligence(XAI).IEEEaccess6(2018),52138–
andhowtheyseektoclarifyconfusion.Futurestudiescouldinvolve 52160.
[3] JuliusAdebayo,JustinGilmer,MichaelMuelly,IanGoodfellow,MoritzHardt,
recruiting participants from diverse countries, regions, and age andBeenKim.2018.SanityChecksforSaliencyMaps.InAdvancesinNeural
groups. InformationProcessingSystems,Vol.21.CurranAssociates,Inc.,9525–9536.
https://doi.org/10.5555/3327546.3327621
[4] JuliusAdebayo,MichaelMuelly,IlariaLiccardi,andBeenKim.2020.Debugging
5 CONCLUSION testsformodelexplanations.InProceedingsofthe34thInternationalConference
onNeuralInformationProcessingSystems.CurranAssociatesInc.,RedHook,
Inourwork,weconductWizard-of-Ozexperimentstoinvestigate NY,USA,Article60,13pages. https://doi.org/10.5555/3495724.3495784
howfree-formconversationsassistusersinunderstandingstaticex- [5] OznurAlkan,DennisWei,MassimilianoMattetti,RahulNair,ElizabethDaly,
andDiptikalyanSaha.2022.FROTE:Feedbackrule-drivenoversamplingfor
planations,promotingtrust,andmakinginformeddecisionsabout
editingmodels.InProceedingsofMachineLearningandSystems,Vol.4.276–301.
AImodels.Participantsengageinconversationalexplanationswith [6] DavidAlvarez-MelisandTommiJaakkola.2017.Acausalframeworkforexplain-
XAIexpertstounderstandhowtheprovidedstaticexplanationex- ingthepredictionsofblack-boxsequence-to-sequencemodels.InProceedingsof
the2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Associ-
plainsthemodeldecision.Toevaluatetheeffectsofconversations, ationforComputationalLinguistics,412–421. https://doi.org/10.18653/v1/D17-
wedesignobjectiveandsubjectivemeasurements.Weobservea 1042
[7] SaleemaAmershi,MayaCakmak,WilliamBradleyKnox,andToddKulesza.
notableimprovementinusers’comprehension,acceptance,trust,
2014.PowertothePeople:TheRoleofHumansinInteractiveMachineLearning.
andcollaborationafterconversations.Fromcollectedconversations, AIMagazine35,4(2014),105–120. https://doi.org/10.1609/aimag.v35i4.2513
wefindthatparticipants’questionsandconfusionsarediverseand [8] DAndersonandKBurnham.2004.Modelselectionandmulti-modelinference.
Springer-Verlag63(2004),512.
unanticipated. Our findings advocate for the integration of dia-
[9] ZahraAshktorab,Q.VeraLiao,CaseyDugan,JamesJohnson,QianPan,Wei
loguesystemsinfutureXAIdesignstoensuremorepersonalized Zhang,SadhanaKumaravel,andMurrayCampbell.2020.Human-AICollabora-
explanations. tioninaCooperativeGameSetting:MeasuringSocialPerceptionandOutcomes.
ProceedingsoftheACMonHuman-ComputerInteraction4(2020),20pages.
[10] TitaAlissaBach,AmnaKhan,HarryHallock,GabrielaBeltrão,andSoniaSousa.
REFERENCES 2022. ASystematicLiteratureReviewofUserTrustinAI-EnabledSystems:
AnHCIPerspective.InternationalJournalofHuman–ComputerInteraction0,0
[1] AshrafAbdul,JoVermeulen,DandingWang,BrianY.Lim,andMohanKankan- (2022),1–16. https://doi.org/10.1080/10447318.2022.2138826
halli.2018.TrendsandTrajectoriesforExplainable,AccountableandIntelligible
TongZhang,X.JessieYang,andBoyangLi
[11] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.2015.Neuralmachine Linguistics,447–459.
translationbyjointlylearningtoalignandtranslate.InProceedingsof3rd [31] FredD.Davis.1989. PerceivedUsefulness,PerceivedEaseofUse,andUser
InternationalConferenceonLearningRepresentations.openreview. AcceptanceofInformationTechnology.MISQuarterly13,3(1989),319–340.
[12] SarahBankins,PaulFormosa,YannickGriep,andDeborahRichards.2022.AI [32] FredDDavis,RichardPBagozzi,andPaulRWarshaw.1989.Useracceptance
DecisionMakingwithDignity?ContrastingWorkers’JusticePerceptionsof ofcomputertechnology:Acomparisonoftwotheoreticalmodels.Management
HumanandAIDecisionMakinginaHumanResourceManagementContext. science35,8(1989),982–1003.
InformationSystemsFrontiers24,3(2022),857–875. [33] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andFei-FeiLi.2009.
[13] GaganBansal,TongshuangWu,JoyceZhou,RaymondFok,BesmiraNushi,Ece ImageNet:Alarge-scalehierarchicalimagedatabase.InProceedingsofthe
Kamar,MarcoTulioRibeiro,andDanielWeld.2021.Doesthewholeexceedits 2009IEEEconferenceoncomputervisionandpatternrecognition.IEEE,248–255.
parts?TheeffectofAIexplanationsoncomplementaryteamperformance.In https://doi.org/10.1109/CVPR.2009.5206848
Proceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems. [34] ElBachirDiop,ShengchuanZhao,andTranVanDuy.2019.Anextensionofthe
AssociationforComputingMachinery,1–16. technologyacceptancemodelforunderstandingtravelers’adoptionofvariable
[14] ShreyasBhat,JosephB.Lyons,CongShi,andX.JessieYang.2024.Evaluating messagesigns.PLoSone14,4(2019).
theImpactofPersonalizedValueAlignmentinHuman-RobotInteraction:In- [35] FinaleDoshi-VelezandBeenKim.2017.Towardsarigorousscienceofinter-
sightsintoTrustandTeamPerformanceOutcomes.InProceedingsofthe2024 pretablemachinelearning.arXivpreprintarXiv:1702.08608(2017).
ACM/IEEEInternationalConferenceonHuman-RobotInteraction.Association [36] FinaleDoshi-Velez,ByronCWallace,andRyanAdams.2015. Graph-Sparse
forComputingMachinery,32–41. https://doi.org/10.1145/3610977.3634921 LDA:Atopicmodelwithstructuredsparsity.InTwenty-NinthAAAIconference
[15] JacobBienandRobertTibshirani.2011.Prototypeselectionforinterpretable onartificialintelligence,Vol.29.AAAIPress,2575–2581. https://doi.org/10.
classification.TheAnnalsofAppliedStatistics5,4(2011),2403–2424. 1609/aaai.v29i1.9603
[16] ArijitBiswasandDeviParikh.2013.Simultaneousactivelearningofclassifiers [37] DannyDriess,FeiXia,MehdiS.M.Sajjadi,CoreyLynch,AakankshaChowd-
&attributesviarelativefeedback.InProceedingsoftheIEEEConferenceon hery,BrianIchter,AyzaanWahid,JonathanTompson,QuanVuong,Tianhe
ComputerVisionandPatternRecognition.IEEE,644–651. Yu,WenlongHuang,YevgenChebotar,PierreSermanet,DanielDuckworth,
[17] FrancescoBodria,FoscaGiannotti,RiccardoGuidotti,FrancescaNaretto,Dino SergeyLevine,VincentVanhoucke,KarolHausman,MarcToussaint,KlausGr-
Pedreschi,andSalvatoreRinzivillo.2023.Benchmarkingandsurveyofexpla- eff,AndyZeng,IgorMordatch,andPeteFlorence.2023.PaLM-E:anembodied
nationmethodsforblackboxmodels.DataMiningandKnowledgeDiscovery multimodallanguagemodel.InProceedingsofthe40thInternationalConference
37,5(2023),1719–1778. https://doi.org/10.1007/s10618-023-00933-9 onMachineLearning.JMLR.org,Article340,20pages.
[18] CarrieJ.Cai,SamanthaWinter,DavidSteiner,LaurenWilcox,andMichaelTerry. [38] SalvatoreD’Avella,GerardoCamacho-Gonzalez,andPaoloTripicchio.2022.On
2019."HelloAI":UncoveringtheOnboardingNeedsofMedicalPractitioners Multi-AgentCognitiveCooperation:CanVirtualAgentsBehavelikeHumans?
forHuman-AICollaborativeDecision-Making.InProceedingsoftheACMon Neurocomputing480,C(2022),27–38. https://doi.org/10.1016/j.neucom.2022.01.
Human-ComputerInteraction,Vol.3.24pages. https://doi.org/10.1145/3359206 025
[19] ColinCamerer,GeorgeLoewenstein,andMartinWeber.1989. Thecurseof [39] UpolEhsan,SamirPassi,QVeraLiao,LarryChan,I-HsiangLee,MichaelMuller,
knowledgeineconomicsettings:Anexperimentalanalysis.JournalofPolitical andMarkORiedl.2024.ThewhoinexplainableAI:HowAIbackgroundshapes
Economy97,5(1989),1232–1254. perceptionsofAIexplanations.InProceedingsoftheCHIConferenceonHuman
[20] ClaudiaCarissoli,LucaNegri,MartaBassi,FabioAlexanderStorm,andAn- FactorsinComputingSystems.AssociationforComputingMachinery,1–32.
tonellaDelleFave.2023.MentalWorkloadandHuman-RobotInteractioninCol- https://doi.org/10.1145/3613904.3642474
laborativeTasks:AScopingReview.InternationalJournalofHuman–Computer [40] JerryAlanFailsandDanROlsenJr.2003.Interactivemachinelearning.InPro-
Interaction0,0(2023),1–20. https://doi.org/10.1080/10447318.2023.2254639 ceedingsofthe8thinternationalconferenceonIntelligentuserinterfaces(Miami,
[21] MatthiasSeliskyCarolinEbermannandStephanWeibelzahl.2023.Explainable Florida,USA).39–45.
AI:TheEffectofContradictoryDecisionsandExplanationsonUsers’Accep- [41] NilsFeldhus,AjayMadhavanRavichandran,andSebastianMöller.2022.Medi-
tanceofAISystems.InternationalJournalofHuman–ComputerInteraction39, ators:ConversationalagentsexplainingNLPmodelbehavior.arXivpreprint
9(2023),1807–1826. https://doi.org/10.1080/10447318.2022.2126812 arXiv:2206.06029(2022).
[22] RichCaruana,YinLou,JohannesGehrke,PaulKoch,MarcSturm,andNoemie [42] ShiFengandJordanBoyd-Graber.2019.WhatCanAIDoforMe?Evaluating
Elhadad.2015.IntelligibleModelsforHealthCare:PredictingPneumoniaRisk MachineLearningInterpretationsinCooperativePlay.InProceedingsofthe24th
andHospital30-DayReadmission.InProceedingsofthe21thACMSIGKDDInter- InternationalConferenceonIntelligentUserInterfaces.AssociationforComputing
nationalConferenceonKnowledgeDiscoveryandDataMining.ACM,1721–1730. Machinery,229–239. https://doi.org/10.1145/3301275.3302265
https://doi.org/10.1145/2783258.2788613 [43] ChristopherFlathmann,BeauG.Schelble,NathanJ.McNeese,BartKnijnenburg,
[23] ChaofanChen,OscarLi,ChaofanTao,AlinaJadeBarnett,JonathanSu,and AnandK.Gramopadhye,andKapilChalilMadathil.2023. ThePurposeful
CynthiaRudin.2019.ThisLookslikeThat:DeepLearningforInterpretableImage PresentationofAITeammates:ImpactsonHumanAcceptanceandPerception.
Recognition.CurranAssociatesInc. InternationalJournalofHuman–ComputerInteraction0,0(2023),1–18. https:
[24] YuanyuanChen,BoyangLi,HanYu,PengchengWu,andChunyanMiao.2021. //doi.org/10.1080/10447318.2023.2254984
Hydra:Hypergradientdatarelevanceanalysisforinterpretingdeepneural [44] PaulFormosa,WendyRogers,YannickGriep,SarahBankins,andDeborah
networks.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.35. Richards.2022. MedicalAIandhumandignity:Contrastingperceptionsof
MITPress,7081–7089. humanandartificiallyintelligent(AI)decisionmakingindiagnosticandmedical
[25] Hao-FeiCheng,RuotongWang,ZhengZhang,FionaO’connell,TerranceGray, resourceallocationcontexts.ComputersinHumanBehavior133(2022),107296.
FMaxwellHarper,andHaiyiZhu.2019.Explainingdecision-makingalgorithms https://doi.org/10.1016/j.chb.2022.107296
throughUI:Strategiestohelpnon-expertstakeholders.InProceedingsofthe [45] KatyIlonkaGero,ZahraAshktorab,CaseyDugan,QianPan,JamesJohnson,
2019CHIconferenceonhumanfactorsincomputingsystems.Associationfor WernerGeyer,MariaRuiz,SarahMiller,DavidR.Millen,MurrayCampbell,
ComputingMachinery,1–12. SadhanaKumaravel,andWeiZhang.2020.MentalModelsofAIAgentsina
[26] HerbertH.ClarkandSusanE.Brennan.1991.GroundinginCommunication. CooperativeGameSetting.InProceedingsofthe2020CHIConferenceonHuman
InPerspectivesonSociallySharedCognition,LaurenBResnick,JohnMLevine, FactorsinComputingSystems.AssociationforComputingMachinery,1–12.
andStephanieDTeasley(Eds.).AmericanPsychologicalAssociation,127–149. https://doi.org/10.1145/3313831.3376316
https://doi.org/10.1037/10096-006 [46] AlyssaGlass,DeborahL.McGuinness,andMichaelWolverton.2008.Toward
[27] HerbertH.ClarkandCatherineR.Marshall.1981. DefiniteKnowledgeand establishingtrustinadaptiveagents.InProceedingsofthe13thInternational
MutualKnowledge.InElementsofDiscourseUnderstanding,AravindK.Joshi, ConferenceonIntelligentUserInterfaces(GranCanaria,Spain).227–236. https:
BonnieL.Webber,andIvanA.Sag(Eds.).10–63. //doi.org/10.1145/1378773.1378804
[28] PauloCortezandMarkJEmbrechts.2013. Usingsensitivityanalysisand [47] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng,
visualizationtechniquestoopenblackboxdataminingmodels.Information Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023.
Sciences225(2013),1–17. MultiModal-GPT:AVisionandLanguageModelforDialoguewithHumans.
[29] DaniloCroce,DanieleRossini,andRobertoBasili.2019.Auditingdeeplearning arXiv:2305.04790[cs.CV]
processesthroughkernel-basedexplanatorymodels.InProceedingsofthe2019 [48] AnaValeriaGonzález,GaganBansal,AngelaFan,YasharMehdad,RobinJia,
ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9th andSrinivasanIyer.2021.DoExplanationsHelpUsersDetectErrorsinOpen-
InternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP). DomainQA?AnEvaluationofSpokenvs.VisualExplanations.InFindingsofthe
AssociationforComputationalLinguistics,4037–4046. AssociationforComputationalLinguistics:ACL-IJCNLP2021,ChengqingZong,
[30] MarinaDanilevsky,KunQian,RanitAharonov,YannisKatsis,BanKawas,and FeiXia,WenjieLi,andRobertoNavigli(Eds.).AssociationforComputational
PrithvirajSen.2020.ASurveyoftheStateofExplainableAIforNaturalLan- Linguistics,1103–1116. https://doi.org/10.18653/v1/2021.findings-acl.95
guageProcessing.InProceedingsofthe1stConferenceoftheAsia-PacificChapter [49] MouadhGuesmi,MohamedAmineChatti,ShoebJoarder,QuratUlAin,Rawaa
oftheAssociationforComputationalLinguisticsandthe10thInternationalJoint Alatrash,ClaraSiepmann,andTannazVahidi.2023.InteractiveExplanation
ConferenceonNaturalLanguageProcessing.AssociationforComputational
BenefitsofConversationsinNeuralNetworkExplainability
withVaryingLevelofDetailsinanExplainableScientificLiteratureRecom- [69] JohannesKunkel,TimDonkers,LisaMichael,Catalin-MihaiBarbu,andJürgen
menderSystem. InternationalJournalofHuman–ComputerInteraction0,0 Ziegler.2019.LetMeExplain:ImpactofPersonalandImpersonalExplanations
(2023),1–22. https://doi.org/10.1080/10447318.2023.2262797 onTrustinRecommenderSystems.InProceedingsofthe2019CHIConferenceon
[50] YaohuiGuo,X.JessieYang,andCongShi.2023.EnablingTeamofTeams:ATrust HumanFactorsinComputingSystems.AssociationforComputingMachinery,
InferenceandPropagation(TIP)ModelinMulti-HumanMulti-RobotTeams. 1–12. https://doi.org/10.1145/3290605.3300717
InRobotics:ScienceandSystemsXIX.AssociationforComputingMachinery, [70] VivianLaiandChenhaoTan.2019.OnHumanPredictionswithExplanations
639–643. https://doi.org/10.15607/RSS.2023.XIX.003 andPredictionsofMachineLearningModels:ACaseStudyonDeception
[51] TaehyunHaandSangyeonKim.2023. ImprovingTrustinAIwithMiti- Detection.InProceedingsoftheConferenceonFairness,Accountability,and
gatingConfirmationBias:EffectsofExplanationTypeandDebiasingStrat- Transparency.AssociationforComputingMachinery,29–38. https://doi.org/
egyforDecision-MakingwithExplainableAI. InternationalJournalofHu- 10.1145/3287560.3287590
man–ComputerInteraction0,0(2023),1–12. https://doi.org/10.1080/10447318. [71] HimabinduLakkaraju,StephenHBach,andJureLeskovec.2016.Interpretable
2023.2285640 decisionsets:Ajointframeworkfordescriptionandprediction.InProceedings
[52] RenateHäuslschmid,MaxvonBülow,BastianPfleging,andAndreasButz.2017. ofthe22ndACMSIGKDDinternationalconferenceonknowledgediscoveryand
SupportingTrustinAutonomousDriving.InProceedingsofthe22ndInternational datamining.1675–1684.
ConferenceonIntelligentUserInterfaces.AssociationforComputingMachinery, [72] HimabinduLakkaraju,DylanSlack,YuxinChen,ChenhaoTan,andSameer
319–329. https://doi.org/10.1145/3025171.3025198 Singh.2022.RethinkingExplainabilityasaDialogue:APractitioner’sPerspec-
[53] XinHe,YeyiHong,XiZheng,andYongZhang.2023. WhatAretheUsers’ tive.arXivpreprintarXiv:2202.01875(2022).
Needs?DesignofaUser-CenteredExplainableArtificialIntelligenceDiagnostic [73] YunshiLan,GaoleHe,JinhaoJiang,JingJiang,WayneXinZhao,andJi-
System. InternationalJournalofHuman–ComputerInteraction39,7(2023), Rong Wen. 2021. A Survey on Complex Knowledge Base Question An-
1519–1542. https://doi.org/10.1080/10447318.2022.2095093 swering:Methods,ChallengesandSolutions.InProceedingsoftheThirti-
[54] SaritaHerse,JonathanVitale,andMary-AnneWilliams.2023. UsingAgent ethInternationalJointConferenceonArtificialIntelligence,IJCAI-21.Interna-
FeaturestoInfluenceUserTrust,DecisionMakingandTaskOutcomeduring tionalJointConferencesonArtificialIntelligenceOrganization,4483–4491.
Human-AgentCollaboration.InternationalJournalofHuman–ComputerInter- https://doi.org/10.24963/ijcai.2021/611
action39,9(2023),1740–1761. https://doi.org/10.1080/10447318.2022.2150691 [74] RetnoLarasati,AnnaDeLiddo,andEnricoMotta.2020.Theeffectofexplana-
[55] RobertRHoffman,ShaneTMueller,GaryKlein,andJordanLitman.2018.Met- tionstylesonuser’strust.In2020WorkshoponExplainableSmartSystemsfor
ricsforexplainableAI:Challengesandprospects.arXivpreprintarXiv:1812.04608 AlgorithmicTransparencyinEmergingTechnologies.AssociationforComputing
(2018). Machinery.
[56] FredHohman,AndrewHead,RichCaruana,RobertDeLine,andStevenM. [75] PiyawatLertvittayakumjorn,LuciaSpecia,andFrancescaToni.2020. FIND:
Drucker.2019. Gamut:ADesignProbetoUnderstandHowDataScientists Human-in-the-LoopDebuggingDeepTextClassifiers.InProceedingsofthe
UnderstandMachineLearningModels.InProceedingsofthe2019CHIConference 2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)
onHumanFactorsinComputingSystems(Glasgow,ScotlandUk).Association (online).332–348. https://doi.org/10.18653/v1/2020.emnlp-main.24
forComputingMachinery,1–13. https://doi.org/10.1145/3290605.3300809 [76] WeixinLiang,JamesZou,andZhouYu.2020. ALICE:ActiveLearningwith
[57] LinweiHu,JieChen,VijayanNNair,andAgusSudjianto.2018. Locallyin- ContrastiveNaturalLanguageExplanations.InProceedingsofthe2020Confer-
terpretablemodelsandeffectsbasedonsupervisedpartitioning(LIME-SUP). enceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP)(online).
arXivpreprintarXiv:1806.00663(2018). 4380–4391. https://doi.org/10.18653/v1/2020.emnlp-main.355
[58] MaximilianIdahl,LijunLyu,UjwalGadiraju,andAvishekAnand.2021.Towards [77] Q.VeraLiao,DanielGruen,andSarahMiller.2020.QuestioningtheAI:Inform-
BenchmarkingtheUtilityofExplanationsforModelDebugging.InProceedings ingDesignPracticesforExplainableAIUserExperiences.InProceedingsofthe
oftheFirstWorkshoponTrustworthyNaturalLanguageProcessing.Associa- 2020CHIConferenceonHumanFactorsinComputingSystems(Honolulu,HI,
tionforComputationalLinguistics,68–73. https://doi.org/10.18653/v1/2021. USA).1–15.
trustnlp-1.8 [78] QVeraLiaoandKushRVarshney.2021.Human-centeredexplainableAI(XAI):
[59] AlexeyIgnatiev,NinaNarodytska,andJoaoMarques-Silva.2019.Abduction- Fromalgorithmstouserexperiences.arXivpreprintarXiv:2110.10790(2021).
basedexplanationsformachinelearningmodels.InProceedingsoftheAAAI [79] BrianYLim,AnindKDey,andDanielAvrahami.2009. Whyandwhynot
ConferenceonArtificialIntelligence.1511–1519. explanationsimprovetheintelligibilityofcontext-awareintelligentsystems.
[60] AlonJacoviandYoavGoldberg.2020.TowardsFaithfullyInterpretableNLP InProceedingsoftheSIGCHIconferenceonhumanfactorsincomputingsystems.
Systems:HowShouldWeDefineandEvaluateFaithfulness?.InProceedingsof AssociationforComputingMachinery,2119–2128.
the58thAnnualMeetingoftheAssociationforComputationalLinguistics(Online). [80] HanLiu,VivianLai,andChenhaoTan.2021.Understandingtheeffectofout-
AssociationforComputationalLinguistics,4198–4205. of-distributionexamplesandinteractiveexplanationsonhuman-aidecision
[61] JeyaVikranthJeyakumar,JosephNoor,Yu-HsiCheng,LuisGarcia,andMani making. ProceedingsoftheACMonHuman-ComputerInteraction5,CSCW2
Srivastava.2020. HowCanIExplainThistoYou?AnEmpiricalStudyof (2021),1–45.
DeepNeuralNetworkExplanationMethods.InAdvancesinNeuralInformation [81] LiLiu,FuGuo,ZishuaiZou,andVincentG.Duffy.2022.Application,Develop-
ProcessingSystems,Vol.33.4211–4222. https://doi.org/10.5555/3495724.3496078 mentandFutureOpportunitiesofCollaborativeRobots(Cobots)inManufactur-
[62] Amir-HosseinKarimi,GillesBarthe,BorjaBalle,andIsabelValera.2020.Model- ing:ALiteratureReview.InternationalJournalofHuman–ComputerInteraction
agnosticcounterfactualexplanationsforconsequentialdecisions.InInterna- 0,0(2022),1–18. https://doi.org/10.1080/10447318.2022.2041907
tionalConferenceonArtificialIntelligenceandStatistics.PMLR,895–905. [82] NinghaoLiu,XiaoHuang,JundongLi,andXiaHu.2018. Oninterpretation
[63] J.F.Kelley.1984.AnIterativeDesignMethodologyforUser-FriendlyNatural ofnetworkembeddingviataxonomyinduction.InProceedingsofthe24th
LanguageOfficeInformationApplications.ACMTransactionsonInformation ACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining
Systems2,1(1984),26–41. https://doi.org/10.1145/357417.357420 (London,UnitedKingdom).1812–1820.https://doi.org/10.1145/3219819.3220001
[64] BeenKim,RajivKhanna,andOluwasanmiKoyejo.2016. ExamplesAreNot [83] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,
Enough,LearntoCriticize!CriticismforInterpretability.InProceedingsofthe andBainingGuo.2021. Swintransformer:Hierarchicalvisiontransformer
30thInternationalConferenceonNeuralInformationProcessingSystems.Curran usingshiftedwindows.InProceedingsoftheIEEE/CVFinternationalconference
AssociatesInc.,2288–2296. oncomputervision.ComputerVisionFoundation,10012–10022.
[65] SunnieS.Y.Kim,ElizabethAnneWatkins,OlgaRussakovsky,RuthFong,and [84] TaniaLombrozo.2006.Thestructureandfunctionofexplanations.Trendsin
AndrésMonroy-Hernández.2023."HelpMeHelptheAI":UnderstandingHow cognitivesciences10,10(2006),464–470.
ExplainabilityCanSupportHuman-AIInteraction.InProceedingsofthe2023CHI [85] ScottMLundbergandSu-InLee.2017. Aunifiedapproachtointerpreting
ConferenceonHumanFactorsinComputingSystems.AssociationforComputing modelpredictions.Advancesinneuralinformationprocessingsystems30(2017).
Machinery,17pages. https://doi.org/10.1145/3544548.3581001 https://doi.org/10.5555/3295222.3295230
[66] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, [86] ManLuo,ZhiyuanFang,TejasGokhale,YezhouYang,andChittaBaral.2023.
KristofT.Schütt,SvenDähne,DumitruErhan,andBeenKim.2019. The End-to-endKnowledgeRetrievalwithMulti-modalQueries.InProceedingsof
(Un)reliabilityofSaliencyMethods. SpringerInternationalPublishing,267– the61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
280. 1:LongPapers)(Toronto,Canada).AssociationforComputationalLinguistics,
[67] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.2012.Imagenetclas- 8573–8589. https://doi.org/10.18653/v1/2023.acl-long.478
sificationwithdeepconvolutionalneuralnetworks.InAdvancesinneural [87] RuikunLuo,NaDu,andX.JessieYang.2022.EvaluatingEffectsofEnhanced
informationprocessingsystems,Vol.25.84–90. https://doi.org/10.1145/3065386 AutonomyTransparencyonTrust,Dependence,andHuman-AutonomyTeam
[68] Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. PerformanceoverTime.InternationalJournalofHuman–ComputerInteraction
2015. PrinciplesofExplanatoryDebuggingtoPersonalizeInteractiveMa- 38,18-20(2022),1962–1971. https://doi.org/10.1080/10447318.2022.2097602
chineLearning.InProceedingsofthe20thInternationalConferenceonIntelli- [88] ShuaiMa,YingLei,XinruWang,ChengboZheng,ChuhanShi,MingYin,and
gentUserInterfaces.AssociationforComputingMachinery,126–137. https: XiaojuanMa.2023. WhoShouldITrust:AIorMyself?LeveragingHuman
//doi.org/10.1145/2678025.2701399 andAICorrectnessLikelihoodtoPromoteAppropriateTrustinAI-Assisted