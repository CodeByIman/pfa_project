Algorithms for Estimating Information Distance
with Application to Bioinformatics and Linguistics
Alexei Kaltchenko
Department of Physics and Computing
Wilfrid Laurier University
Waterloo, Ontario N2L3C5, Canada
akaltche@wlu.ca
Abstract where K(x|y) is the conditional Kolmogorov
After reviewing unnormalized and normalized complexity[8] of string x relative to string y, defined as
information distances based on incomputable notions of the length of a shortest binary program to compute x if y
Kolmogorov complexity, we discuss how Kolmogorov is furnished as an auxiliary input to the computation.
complexity can be approximated by data compression They shown that the distance E 1 (x,y) is a universal
algorithms. We argue that optimal algorithms for data metric. Formally, a distance function d with nonnegative
compression with side information can be successfully real values, defined on the Cartesian product X(cid:215) X of a
used to approximate the normalized distance. Next, we set X, is called a metric on X if for every x, y, z ∈ X:
discuss an alternative information distance, which is • d(x,y) = 0 iff x = y (identity axiom)
based on relative entropy rate (also known as Kullback- • d(x,y) + d(y,z) ≥ d(x,z) (triangle inequality)
Leibler divergence), and compression-based algorithms • d(x,y) = d(y,x) (symmetry axiom)
for its estimation. Based on available biological and The universality implies that if two objects are similar in
linguistic data, we arrive to unexpected conclusion that some computable metric, then they are at least that
in Bioinformatics and Computational Linguistics this similar in E (x,y) sense.
1
alternative distance is more relevant and important than A distance function is called normalized if it takes values
the ones based on Kolmogorov complexity. in [0;1]. Thus, distance E (x,y) is clearly unnormalized.
1
Li at al argued[7] that in Bioinformatics an unnormalized
Keywords: Information distance; bioinformatics; distance may not be a proper evolutionary distance
Kolmogorov complexity; entropy estimation; conditional measure. It would put two long and complex sequences
entropy; relative entropy; Kullback-Leibler divergence; that differ only by a tiny fraction of the total information
divergence estimation; data compression; side as dissimilar as two short sequences that differ by the
information. same absolute amount and are completely random with
respect to one another. They proposed a normalized
information distance E (x,y) defined by
2
1. INFORMATION DISTANCES BASED max{K(x| y),K(y|x)}
E (x,y)! (1)
ON KOLMOGOROV COMPLEXITY 2 max{K(x),K(y)}
Suppose, for a positive integer n, we have two strings of
and proved that it is a universal metric, too.
characters x!(x ,x ,x ,…,x ) and
1 2 3 n
y!(y ,y ,y ,…,y ) that describe two similar objects 2. ESTIMATION OF DISTANCE E (x,y)
1 2 3 n 2
(such as DNA sequences, texts, pictures, etc). The strings VIA COMPRESSION.
are assumed to be drawn from the same alphabet A. To
Since the information distance E (.,.) is based on
quantify similarity between the objects, one needs a 2
noncomputable notions of Kolmogorov complexities, we
notion of information distance between two individual
need to approximate the latter by computable means. It is
objects (strings). Bennett et al introduced[3] a distance
well known that the Kolmogorov complexity and
function E (x,y) defined by
1 compressibility of strings are closely related[6],[8]. So
E (x,y)!max{K(x| y),K(y|x)},
1 we need data compression algorithms suitable for
approximating Kolmogorov complexities.
CCECE 2004- CCGEI 2004, Niagara Falls, May/mai 2004
0-7803-8253-6/04/$17.00 '2004 IEEE
To express function E (x,y) via information- In [7], Li et al used GenCompress[5], an efficient
2
algorithm for DNA sequence compression, for estimating
theoretic measures relevant to data compression, we can
Kolmogorov complexities. Based on good compression
write distance E (x,y) as
2 performance of GenCompress, we can reasonably
max{1K(x| y),1K(y|x)}
assume that the algorithm is optimal or near-optimal,
E (x,y)= n n (2)
2 max{1K(x),1K(y)} and, thus, the rate |b
x
|/|x|, provides a good estimate
n n
Through the rest of this paper we assume1 that strings x forH(X)≈ | 1 x| K(x). As for estimating joint Kolmogorov
and y are generated by finite-order, stationary Markov complexityK(x,y), Li et al used the concatenated
sources X and Y, respectively, and this source pair jointly
sequence x+ y!(x ,x ,x ,…,x ,y ,y ,y ,…,y )as
forms a finite-order, stationary Markov source, too. 1 2 3 n 1 2 3 n
Then, from Information Theory, we have the following the input to GenCompress as shown in Figure 2. They
almost sure convergence: heuristically assumed that the size of the compressed
lim1K(x)=H(X), a.s. output would approximateK(x,y), which would imply
n→∞n
(3) that the ratio |b |/|x| would approximate H(X,Y).
lim1K(x| y)=H(X |Y), a.s. x+y
n→∞n
x+y b (binary codeword) x+y
Thus, we have x+y
Encoder Decoder
max{H(X |Y),H(Y |X)} |b |/|x| ≈ / H(X,Y)
x+y
limE (x,y)= , a.s. (4)
n→∞ 2 max{H(X),H(Y)}
Figure 2: Non-optimal compression of string pairs.
Consequently, the right-hand side of (4) can be used as a
However, from Information Theory, this assumption is
good approximation of E (x,y) for sufficiently large n.
2 generally not correct because encoding the concatenated
Thus, we are interested in data compression algorithms
string x+y as a single string from the same alphabet A
capable of estimating entropy rate H(i) and conditional
does not properly utilize the correlation between x and y.
entropy rate H(i|i). In view of the information-theoretic
To properly utilize the correlation, a compression
identity algorithm must encode the (x,y) pair as the string
H(X |Y)=H(X,Y)−H(Y), (5)
x  x  x  x 
where H(X,Y)denotes joint entropy rate, we can also s! 1 , 2 , 3 ,…, n  of supersymbols from
y
1
 y
2
 y
3
 y
n

estimate conditional entropy rate H(i|i) indirectly, via
the alphabet A(cid:215)A"
estimation of joint entropy rate H(i,i) and
(unconditional) entropy rate H(i). In the following three
2.2 Estimation of Joint Entropy Rate
subsections, we will discuss data compression algorithms
for estimation H(i), H(i,i), and H(i|i), respectively.
H(X,Y)can be approximated by the compression rate of
an optimal lossless data compression algorithm as shown
2.1 Estimation of Entropy Rate in Figure 3. From Information Theory, the optimality
For any data compression algorithm, we define its implies that, as the length of x and y grows, |b |/|x|
x,y
compression rate by the ratio |b |/|x|, where b is the converges toH(X,Y).
x x
binary codeword produced by the algorithm for input
x b (binary codeword) x
string x, and |⋅| denotes string length. Then,H(X)can x,y
Encoder Decoder
y |b |/|x| ≈ H(X,Y) y
be approximated by the compression rate of an optimal x,y
lossless data compression algorithm as shown in
Figure 3: Optimal compression of string pairs.
Figure 1. From Information Theory, the optimality
implies that, as the length of x grows, |b |/|x| converges As we discussed in the previous section, for its optimal
x
toH(X). compression, a string pair (x,y) must be encoded as a
x b x (binary codeword) x string s!     x 1  ,   x 2  ,   x 3  ,…,   x n     of supersymbols
Encoder |b |/|x|≈ Η(X) Decoder y 1  y 2  y 3  y n 
x
from the alphabet A(cid:215)A Moreover, we can simply
Figure 1: Optimal compression of strings
compress string s by any optimal algorithm for string
compression depicted in Figure 1. Then, from
Information Theory, we have: b /|s|≈H(X,Y)
s
1 This is a common assumption for compression analysis.
2.3 Estimation of Conditional Entropy Rate coincide. Thus, D(Z||X) can be naturally viewed as a
via Compression with Side Information distance between the measures p and q However,
x z.
D(⋅||⋅)is not a metric because it generally is neither
H(X |Y)can be approximated by the compression rate symmetric, nor satisfies the triangle inequality.
of an optimal lossless data compression algorithm with
side information as shown in Figure 4. From Information It is not difficult to see that we can haveD(Z||X)equal
Theory, the optimality implies that, as the length of x and
zero while the conditional entropy rate H(Z|X)is large
y grows, |b |/|x| converges toH(X |Y).
x|y and vice versa. Thus, an information distance based on
x b x|y (binary codeword) x relative entropy rate may be a complement or even an
Encoder Decoder alternative to an information distance based on
|b |/|x| ≈ H(X|Y)
y x|y Kolmogorov complexity.
(side information)
3.1 Estimation of Relative Entropy via Data
Figure 4: Optimal compression of string x with
Compression
string y used as side information.
In data compression, the relative entropy D(Z||X)has
An optimal (in the above convergence sense) algorithm
the following interpretation of compression non-
for universal lossless data compression with side
optimality. Loosely speaking, if we have a compression
information was proposed and analyzed in [9]. It is
code (mapping), which optimally compresses string x
important to note that, as in the case with estimation of
and use this code to (non-optimally) compress string z,
joint entropy rate, the algorithm encodes a string pair
then the compression rate will be H(Z)+D(Z||X). On
(x,y) as a string of supersymbols from the alphabet
the other hand, a compression code, which is optimal for
A(cid:215)A
string z, will compress it at the rate of H(Z). Thus, by
subtracting the latter from the former, we will get an
2.3 Estimation of Conditional Entropy Rate:
estimate of D(Z||X). Compression-based algorithms
Direct vs. Indirect
for estimation of D(⋅||⋅) were proposed and analyzed in
While H(X,Y)is never less than H(Y), the estimate of
[4] and [10]. Work [4] also includes many interesting
H(X,Y)can be less than the estimate of H(Y) for some
simulation results. Yet another compression-based
string pairs, especially for those not sufficiently long. algorithm for D(⋅||⋅) estimation was introduced in [2]. It
Thus, we can get a negative estimate of conditional
was however purely heuristic and there was no claim that
entropy rate in the indirect estimation via identity (5).
the algorithm converges to D(⋅||⋅).
This may result in a negative information distance, which
Conjecture:
in turn would adversely affect building distance-based
If we consider non-optimal compression of a
phylogeny trees in Bioinformatics and Computational
concatenated string x+y by GenCompress discussed in
Linguistics. Clearly, the direct estimation of conditional
Section 2.1, then, for sufficiently large strings,
entropy H(X |Y) via compression with side information
b / x ≈H(X)+H(Y)+D(Y ||X).
will never yield a negative value (regardless of the x+y
estimation accuracy). Thus, instead of K(x| y), Li et al[7] could have actually
estimated
b / x − b / y ≈H(X)+D(Y ||X).
3. INFORMATION DISTANCE BASED ON x+y y
They used this estimates to build (1) the mammalian
RELATIVE ENTROPY RATE
DNA evolutionary tree and (2) the language
classification tree, where each language was represented
Let p andq be probability measures for sources X and Z,
x z by a file of (cid:147)The Universal Declaration of Human
respectively. The relative entropy rateD(Z||X) (also
Rights(cid:148) in that language. Since the Declaration was
known as Kullback-Leibler divergence) is defined by originally created in English and then (cid:147)losslessly(cid:148)
1 q (x) translated into the other languages, the value H(X)for
D(Z||X)!limsup ∑q (x)log z .
n z p (x) every language is approximately the same[1]. Thus,
n→∞ x x
instead ofE (x,y), Li et al could have actually estimated
D(Z||X)is a nonnegative continuous function 2
(functional) and equals to zero if and only if p and q max{D(Y ||X),D(X ||Y)}+const.
x z
Our conjecture is directly supported by the computational [4] H. Cai, S. Kulkarni, and S. Verdu, ‘‘Universal Estimation of
results in [2],[4], where the language classification tree Entropy and Divergence Via Block Sorting,’’ Proc. of the
was built with symmetrized information distance 2002 IEEE Intern. Symp. Inform. Theory, p.433, USA,
2002.
function based on relative entropy. The conjecture is also
indirectly supported by the following fact. The
[5] X. Chen, S. Kwong, M. Li, (cid:147)A compression algorithm for
mammalian DNA evolutionary tree, which was built in
DNA Sequences(cid:148), IEEE-EMB Special Issue on
[7] based on presumably incorrect estimates of K(i|i), is Bioinformatics, Vol. 20, No. 4, pp. 61(cid:150) 66, 2001.
nevertheless correct. Thus, in Bioinformatics and
Linguistics, an information distance based on relative [6] J. Kieffer, En-hui Yang, (cid:147)Sequential codes, lossless
compression of individual sequences, and Kolmogorov
entropy appears to be more meaningful than a distance
complexity(cid:148), IEEE Trans. Inform. Theory, Vol. 42, No. 1,
based on Kolmogorov complexity.
pp. 29 (cid:150) 39, 1996.
6. CONCLUSIONS [7] M. Li, X. Chen, X. Li, B. Ma, and P. Vitanyi, "The
similarity metric", the Proceedings of the 14th annual
ACM-SIAM symposium on Discrete algorithms (SODA),
After reviewing unnormalized and normalized
pp. 863 (cid:150) 872, 2003.
information distances based on notions of Kolmogorov
complexity, we propose compression algorithms for
[8] M. Li and P. VitÆnyi, An Introduction to Kolmogorov
Kolmogorov complexity estimation. In particular, we
Complexity and Its Applications, Springer, New York,
suggest a method for direct estimation of conditional 1997, 2nd ed.
Kolmogorov complexity, which always yields a
nonnegative value. We point out the limitations of the [9] En-hui Yang, A. Kaltchenko, and J. Kieffer, ‘‘Universal
approach for estimating joint Kolmogorov complexity lossless data compression with side information by using a
presented in [7]. conditional MPM grammar transform,’’ IEEE Trans.
Inform. Theory, Vol. 47, No. 6, pp. 2130 (cid:150) 2150,
We also discuss an alternative information distance based
Sep. 2001.
on relative entropy rate (also known as Kullback-Leibler
divergence) and compression-based algorithms for its
[10] J. Ziv and N. Merhav, ‘‘A measure of relative entropy
estimation.
between individual sequences with application to universal
Based on the computational results for the DNA classification’’, IEEE Trans. Inform. Theory, Vol. 39,
evolutionary tree[7] and for the language classification No. 4, pp. 1270 (cid:150)1279, July 1993.
tree[2],[4],[7], we conjecture that in Bioinformatics and
Linguistics, an information distance based on relative
entropy rate is more relevant and important than
distances based on Kolmogorov complexity.
Acknowledgements
The author thanks Ming Li for valuable comments on
this work.
References
[1] F. Behr, V. Fossum, M. Mitzenmacher, and D. Xiao,
(cid:147)Estimating and Comparing Entropies Across Written
Natural Languages Using PPM Compression(cid:148), the 2003
Data Compression Conference (DCC 2003), p. 416, USA.
[2] D. Benedetto, E. Caglioti, and V. Loreto, (cid:147)Language Trees
and Zipping(cid:148), Physical Review Letters, Vol. 88, No. 4, Jan
2002.
[3] C. Bennett, P. Gacs, M. Li, P. Vitanyi, W. Zurek,
(cid:147)Information distance(cid:148), ’’ IEEE Trans. Inform. Theory,
Vol. 44, No. 4 , pp. 1407 (cid:150) 1423, July 1998.