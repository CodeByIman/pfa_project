"""
Pre-ranking Module with Three Summarization Methods

This module implements pre-ranking using TF-IDF, LSA, and Abstractive summarization.
Each paper gets three types of summaries for comprehensive analysis.
"""

from typing import List, Dict
from dataclasses import dataclass
from pathlib import Path
import numpy as np

from ..retrieval.arxiv_client import Paper
from ..pdf_processing.downloader import download_pdf
from ..pdf_processing.extractor import extract_text_from_pdf
from ..pdf_processing.preprocess import clean_scientific_text
from ..generation.tfidf_summarizer import summarize_tfidf
from ..generation.lsa_summarizer import summarize_lsa
from ..generation.abstractive_summarizer import summarize_abstractive, generate_final_response_mistral, _check_ollama_availability
from .embedding import embed_texts, cosine_similarity_matrix


@dataclass
class RankedPaper:
	"""
	Represents a ranked paper with multiple types of summaries and final response.
	
	Attributes:
		paper: The original paper object
		score: Relevance score from semantic similarity
		summary_tfidf: TF-IDF extractive summary
		summary_lsa: LSA extractive summary  
		summary_abstractive: Abstractive summary using transformers
		summary_combined: Combined summary for ranking
		final_response: Human-like response generated by Mistral meta-processor
		method_used: The summarization method used for the primary summary
	"""
	paper: Paper
	score: float
	summary_tfidf: str
	summary_lsa: str
	summary_abstractive: str
	summary_combined: str
	final_response: str
	method_used: str


def pre_rank_papers_abstract_only(query: str, papers: List[Paper], top_k: int = 5, summary_mode: str = "auto", use_mistral_final_response: bool = True) -> List[RankedPaper]:
	"""
	Fast ranking using only abstracts with configurable summarization methods.
	
	Args:
		query: Search query for ranking
		papers: List of papers to rank
		top_k: Number of top papers to return
		summary_mode: Summarization mode ("tfidf", "lsa", "abstractive", "mistral", "auto")
	
	Returns:
		List of ranked papers with multiple types of summaries
	"""
	if not papers:
		return []
	
	# Use abstracts directly for summarization and ranking
	selected: List[RankedPaper] = []
	for paper in papers:
		abstract = paper.abstract
		if not abstract or len(abstract.strip()) < 50:
			continue
		
		# Generate all technical summaries (no longer using Mistral as a direct summarizer)
		sum_tfidf = summarize_tfidf(abstract, max_sentences=2)
		sum_lsa = summarize_lsa(abstract, max_sentences=2)
		sum_abstractive = summarize_abstractive(abstract, max_length=150, min_length=40)
		
		# Determine primary summary for ranking based on mode
		if summary_mode == "abstractive" or summary_mode == "auto":
			combo_summary = sum_abstractive
			method_used = "abstractive"
		elif summary_mode == "tfidf":
			combo_summary = sum_tfidf
			method_used = "tfidf"
		elif summary_mode == "lsa":
			combo_summary = sum_lsa
			method_used = "lsa"
		else:
			# Default fallback
			combo_summary = (sum_tfidf + "\n" + sum_lsa).strip()
			method_used = "combined"
		
		# Generate final human-like response using Mistral meta-processor (optional for speed)
		if use_mistral_final_response:
			final_response = generate_final_response_mistral(
				title=paper.title,
				authors=paper.authors,
				year=str(paper.year),
				abstract=abstract,
				tfidf_summary=sum_tfidf,
				lsa_summary=sum_lsa,
				abstractive_summary=sum_abstractive
			)
		else:
			# Fast fallback without Mistral call
			final_response = f"This paper '{paper.title}' by {', '.join(paper.authors[:2])} ({paper.year}) presents research findings based on the analysis summaries."
		
		selected.append(RankedPaper(
			paper=paper, 
			score=0.0, 
			summary_tfidf=sum_tfidf,
			summary_lsa=sum_lsa,
			summary_abstractive=sum_abstractive,
			summary_combined=combo_summary,
			final_response=final_response,
			method_used=method_used
		))

	if not selected:
		return []

	# Rank by semantic similarity using combined summary
	query_emb = embed_texts([query])
	sum_texts = [s.summary_combined for s in selected]
	sum_emb = embed_texts(sum_texts)
	sim = cosine_similarity_matrix(query_emb, sum_emb)[0]
	
	for i, sc in enumerate(sim):
		selected[i].score = float(sc)

	selected.sort(key=lambda x: x.score, reverse=True)
	return selected[:top_k]


def pre_rank_papers(query: str, papers: List[Paper], data_dir: Path, max_pdfs: int = 10, top_k: int = 5, summary_mode: str = "auto", use_mistral_final_response: bool = True) -> List[RankedPaper]:
	"""
	Full ranking with PDF processing and configurable summarization methods.
	
	Args:
		query: Search query for ranking
		papers: List of papers to rank
		data_dir: Directory for caching PDFs
		max_pdfs: Maximum number of PDFs to download and process
		top_k: Number of top papers to return
		summary_mode: Summarization mode ("tfidf", "lsa", "abstractive", "mistral", "auto")
	
	Returns:
		List of ranked papers with multiple types of summaries
	"""
	selected: List[RankedPaper] = []
	data_dir.mkdir(parents=True, exist_ok=True)
	pdf_dir = data_dir / 'pdfs'
	pdf_dir.mkdir(parents=True, exist_ok=True)

	for paper in papers[:max_pdfs]:
		text = paper.abstract  # Default to abstract
		
		# Try to get full text from PDF if available
		if paper.pdf_url:
			pdf_path = download_pdf(paper.pdf_url, pdf_dir)
			if pdf_path is not None:
				extracted_text = extract_text_from_pdf(pdf_path, max_pages=8)
				if extracted_text and len(extracted_text.strip()) >= 200:
					text = extracted_text
		
		# Clean the text
		text = clean_scientific_text(text)
		if not text or len(text.strip()) < 50:
			continue
			
		# Generate all technical summaries (no longer using Mistral as a direct summarizer)
		sum_tfidf = summarize_tfidf(text, max_sentences=3)
		sum_lsa = summarize_lsa(text, max_sentences=3)
		sum_abstractive = summarize_abstractive(text, max_length=150, min_length=40)
		
		# Determine primary summary for ranking based on mode
		if summary_mode == "abstractive" or summary_mode == "auto":
			combo_summary = sum_abstractive
			method_used = "abstractive"
		elif summary_mode == "tfidf":
			combo_summary = sum_tfidf
			method_used = "tfidf"
		elif summary_mode == "lsa":
			combo_summary = sum_lsa
			method_used = "lsa"
		else:
			# Default fallback
			combo_summary = (sum_tfidf + "\n" + sum_lsa).strip()
			method_used = "combined"
		
		# Generate final human-like response using Mistral meta-processor (optional for speed)
		if use_mistral_final_response:
			final_response = generate_final_response_mistral(
				title=paper.title,
				authors=paper.authors,
				year=str(paper.year),
				abstract=paper.abstract,  # Use original abstract for context
				tfidf_summary=sum_tfidf,
				lsa_summary=sum_lsa,
				abstractive_summary=sum_abstractive
			)
		else:
			# Fast fallback without Mistral call
			final_response = f"This paper '{paper.title}' by {', '.join(paper.authors[:2])} ({paper.year}) presents research findings based on the analysis summaries."
		
		selected.append(RankedPaper(
			paper=paper, 
			score=0.0, 
			summary_tfidf=sum_tfidf,
			summary_lsa=sum_lsa,
			summary_abstractive=sum_abstractive,
			summary_combined=combo_summary,
			final_response=final_response,
			method_used=method_used
		))

	if not selected:
		return []

	# Rank by semantic similarity using combined summary
	query_emb = embed_texts([query])
	sum_texts = [s.summary_combined for s in selected]
	sum_emb = embed_texts(sum_texts)
	sim = cosine_similarity_matrix(query_emb, sum_emb)[0]
	
	for i, sc in enumerate(sim):
		selected[i].score = float(sc)

	selected.sort(key=lambda x: x.score, reverse=True)
	return selected[:top_k] 